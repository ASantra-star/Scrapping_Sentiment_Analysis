{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu7N8HZWNUWpAM6ZR+z8f7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASantra-star/Scrapping_Sentiment_Analysis/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser requests beautifulsoup4 nltk openpyxl wordcloud seaborn matplotlib"
      ],
      "metadata": {
        "id": "AlL1PYSptqgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "WrqG84uPuofN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# METRIC FUNCTIONS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMN DEFINITIONS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"ARTICLE_TEXT\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# RSS FEEDS (Expanded for 200+ articles)\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\",\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    \"https://www.wired.com/feed/rss\",\n",
        "    \"http://feeds.arstechnica.com/arstechnica/index\",\n",
        "    \"http://feeds.feedburner.com/TechCrunch/\",\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    \"https://www.engadget.com/rss.xml\",\n",
        "    \"https://www.technologyreview.com/feed/\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url, text] + ext_metrics)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# SAVE OUTPUT FILES\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 1. GENERATE WORD CLOUD\n",
        "# ============================================================\n",
        "all_text = \" \".join(df_extended['ARTICLE_TEXT'])\n",
        "#wordcloud = WordCloud(width=1200, height=600, background_color='white', collocations=False).generate(all_text)\n",
        "\n",
        "#plt.figure(figsize=(15,7))\n",
        "#plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#plt.axis('off')\n",
        "#plt.title(\"Word Cloud of All Articles\", fontsize=20)\n",
        "#plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 2. MOST COMPLEX ARTICLES\n",
        "# ============================================================\n",
        "df_extended['COMPLEXITY_SCORE'] = df_extended['COMPLEX WORD COUNT'] * df_extended['FOG INDEX']\n",
        "top_complex = df_extended.sort_values('COMPLEXITY_SCORE', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 Most Complex Articles:\")\n",
        "print(top_complex[['URL_ID','URL','COMPLEX WORD COUNT','FOG INDEX','COMPLEXITY_SCORE']])"
      ],
      "metadata": {
        "id": "VITOZELhx2Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "pred_labels = []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment with Model\"):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        pred_labels.append(\"NEUTRAL\")  # handle empty text\n",
        "    else:\n",
        "        res = sentiment_model(text[:512])[0]  # limit to first 512 tokens for speed\n",
        "        pred_labels.append(res['label'].upper())  # \"POSITIVE\" or \"NEGATIVE\"\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = pred_labels\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "vader_labels = []\n",
        "\n",
        "for text in df_ext['ARTICLE_TEXT']:\n",
        "    s = sia.polarity_scores(str(text))\n",
        "    vader_labels.append(\"POSITIVE\" if s['compound'] >= 0 else \"NEGATIVE\")\n",
        "\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "\n",
        "\n",
        "accuracy = (df_ext['MODEL_SENTIMENT'] == df_ext['VADER_SENTIMENT']).mean()\n",
        "print(f\"Accuracy of VADER sentiment vs pre-trained model: {accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "mismatches = df_ext[df_ext['MODEL_SENTIMENT'] != df_ext['VADER_SENTIMENT']]\n",
        "print(f\"\\nNumber of mismatches: {len(mismatches)}\")\n",
        "print(mismatches[['URL_ID','URL','VADER_SENTIMENT','MODEL_SENTIMENT']])\n"
      ],
      "metadata": {
        "id": "0Vpj_eOJygU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_ext['MODEL_SENTIMENT'], df_ext['VADER_SENTIMENT']))\n"
      ],
      "metadata": {
        "id": "AnqpyaQczBSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# Hugging Face pre-trained sentiment model\n",
        "model_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "model_labels = []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment with Model\"):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        model_labels.append(\"NEUTRAL\")\n",
        "    else:\n",
        "        res = model_analyzer(text[:512])[0]  # limit for speed\n",
        "        model_labels.append(res['label'].upper())\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = model_labels\n",
        "\n",
        "\n",
        "vader_labels = []\n",
        "\n",
        "for text in df_ext['ARTICLE_TEXT']:\n",
        "    s = sia.polarity_scores(str(text))\n",
        "    compound = s['compound']\n",
        "    if compound > 0.05:\n",
        "        vader_labels.append(\"POSITIVE\")\n",
        "    elif compound < -0.05:\n",
        "        vader_labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "\n",
        "accuracy = (df_ext['MODEL_SENTIMENT'] == df_ext['VADER_SENTIMENT']).mean()\n",
        "print(f\"Accuracy of VADER (strong sentiment) vs pre-trained model: {accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "mismatches = df_ext[df_ext['MODEL_SENTIMENT'] != df_ext['VADER_SENTIMENT']]\n",
        "print(f\"\\nNumber of mismatches: {len(mismatches)}\")\n",
        "print(mismatches[['URL_ID','URL','VADER_SENTIMENT','MODEL_SENTIMENT']])"
      ],
      "metadata": {
        "id": "KYYh7WQqzPRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_ext['MODEL_SENTIMENT'], df_ext['VADER_SENTIMENT']))"
      ],
      "metadata": {
        "id": "2nJAJ0kGziSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_probs, model_labels, vader_compound, vader_labels = [], [], [], []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment\"):\n",
        "    if not isinstance(text, str) or len(text.strip())==0:\n",
        "        model_probs.append(0.5)\n",
        "        model_labels.append(\"NEUTRAL\")\n",
        "        vader_compound.append(0)\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "        continue\n",
        "\n",
        "    # Hugging Face model prediction\n",
        "    res = model_analyzer(text[:512])[0]\n",
        "    model_labels.append(res['label'].upper())\n",
        "    model_probs.append(res['score'] if res['label'].upper()=='POSITIVE' else 1-res['score'])\n",
        "\n",
        "    # VADER\n",
        "    s = sia.polarity_scores(text)\n",
        "    compound = s['compound']\n",
        "    vader_compound.append(compound)\n",
        "    if compound > 0.05:\n",
        "        vader_labels.append(\"POSITIVE\")\n",
        "    elif compound < -0.05:\n",
        "        vader_labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = model_labels\n",
        "df_ext['MODEL_PROB'] = model_probs\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "df_ext['VADER_COMPOUND'] = vader_compound\n",
        "\n",
        "\n",
        "#plt.figure(figsize=(10,6))\n",
        "#sns.scatterplot(x='VADER_COMPOUND', y='MODEL_PROB', hue='MODEL_SENTIMENT', data=df_ext, alpha=0.7)\n",
        "#plt.axvline(0.05, color='red', linestyle='--', label='VADER POS threshold')\n",
        "#plt.axvline(-0.05, color='blue', linestyle='--', label='VADER NEG threshold')\n",
        "#plt.xlabel(\"VADER Compound Score\")\n",
        "#plt.ylabel(\"ML Model Positive Probability\")\n",
        "#plt.title(\"VADER Compound Score vs ML Model Sentiment Probability\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "mismatches = df_ext[df_ext['VADER_SENTIMENT'] != df_ext['MODEL_SENTIMENT']]\n",
        "print(f\"Total mismatches: {len(mismatches)}\")\n",
        "display_cols = ['URL_ID','URL','VADER_SENTIMENT','VADER_COMPOUND','MODEL_SENTIMENT','MODEL_PROB']\n",
        "print(mismatches[display_cols].head(10))  # show top 10 mismatches\n",
        "\n",
        "\n",
        "# Extreme mismatch = VADER strongly opposite to model\n",
        "mismatches['EXTREME_DIFF'] = abs(mismatches['VADER_COMPOUND'] - (mismatches['MODEL_PROB']*2-1))\n",
        "top_extreme = mismatches.sort_values('EXTREME_DIFF', ascending=False).head(5)\n",
        "print(\"\\nTop 5 Extreme Mismatches:\")\n",
        "print(top_extreme[display_cols])"
      ],
      "metadata": {
        "id": "BnwaZiXTzyfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IoiYJaI0xrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}