{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfs4QYfIUCdnlmNsIA2Ocg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASantra-star/Scrapping_Sentiment_Analysis/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV65PkLS3Vpr"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 requests nltk pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "sIc_ePpi3YOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_url = \"https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/export?format=xlsx\"\n",
        "input_path = \"/content/Input.xlsx\"\n",
        "\n",
        "r = requests.get(input_url)\n",
        "with open(input_path, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "df_input = pd.read_excel(input_path)\n",
        "df_input"
      ],
      "metadata": {
        "id": "X3pQoem33q64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_words_from_files(file_list):\n",
        "    words = set()\n",
        "    for file in file_list:\n",
        "        with open(file, \"r\", encoding=\"latin-1\") as f:\n",
        "            words.update(word.strip().lower() for word in f)\n",
        "    return words"
      ],
      "metadata": {
        "id": "TyBscbPx3rs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "Uxh5kDlp4s1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# Folder 1\n",
        "folder1_id = \"1rd7YdoX8tED9mujc0c-6evJU4y7LFc_R\"\n",
        "gdown.download_folder(id=folder1_id, output=\"StopWords\", quiet=False)\n",
        "\n",
        "# Folder 2\n",
        "folder2_id = \"1YRcVlJO3ZaC78iTC6JcunfZl7Fz4AL8v\"\n",
        "gdown.download_folder(id=folder2_id, output=\"MasterDictionary\", quiet=False)\n"
      ],
      "metadata": {
        "id": "Ur-UpBi83zK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_files = [\n",
        "    '/content/StopWords/StopWords_Auditor.txt',\n",
        "    '/content/StopWords/StopWords_Currencies.txt',\n",
        "    '/content/StopWords/StopWords_DatesandNumbers.txt',\n",
        "    '/content/StopWords/StopWords_Generic.txt',\n",
        "    '/content/StopWords/StopWords_GenericLong.txt',\n",
        "    '/content/StopWords/StopWords_Geographic.txt',\n",
        "    '/content/StopWords/StopWords_Names.txt'\n",
        "]\n",
        "\n",
        "stop_words = load_words_from_files(stopword_files)"
      ],
      "metadata": {
        "id": "bBDaRQcN31rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = load_words_from_files(['/content/MasterDictionary/positive-words.txt'])\n",
        "negative_words = load_words_from_files(['/content/MasterDictionary/negative-words.txt'])"
      ],
      "metadata": {
        "id": "RM0QrjwI5GTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        article = soup.find('article')\n",
        "        if article:\n",
        "            return article.get_text(separator=' ')\n",
        "        else:\n",
        "            return soup.get_text()\n",
        "    except:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "p0nHp2xb5I9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "pGtIWWhX5MsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "\n",
        "    return max(count, 1)"
      ],
      "metadata": {
        "id": "M6ydWpE65PYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_text(text)\n",
        "\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    positive_score = sum(1 for w in tokens if w in positive_words)\n",
        "    negative_score = sum(1 for w in tokens if w in negative_words)\n",
        "\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    syllables_per_word = sum(count_syllables(w) for w in tokens) / word_count if word_count else 0\n",
        "    avg_word_length = sum(len(w) for w in tokens) / word_count if word_count else 0\n",
        "\n",
        "    pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    return [\n",
        "        positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "        avg_sentence_length, percentage_complex_words, fog_index,\n",
        "        avg_sentence_length, complex_word_count, word_count,\n",
        "        syllables_per_word, pronouns, avg_word_length\n",
        "    ]"
      ],
      "metadata": {
        "id": "HF9mA2I15R59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "NDP3XJGm5svl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_data = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    text = extract_article_text(row['URL'])\n",
        "    metrics = analyze_text(text)\n",
        "\n",
        "    output_data.append([row['URL_ID'], row['URL']] + metrics)"
      ],
      "metadata": {
        "id": "VDrucw6E5Xqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "df_output = pd.DataFrame(output_data, columns=columns)\n",
        "\n",
        "output_path = \"/content/Output.xlsx\"\n",
        "df_output.to_excel(output_path, index=False)\n",
        "\n",
        "df_output\n"
      ],
      "metadata": {
        "id": "jQTtxrE95asX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser openpyxl"
      ],
      "metadata": {
        "id": "w-8C34ad7PhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "QUERY = \"financial markets\"\n",
        "MAX_URLS = 400\n",
        "\n",
        "# Google News RSS feed\n",
        "rss_url = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Fetch URLs from web\n",
        "# ---------------------------\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "urls = []\n",
        "for entry in feed.entries[:MAX_URLS]:\n",
        "    urls.append(entry.link)\n",
        "\n",
        "# ---------------------------\n",
        "# Auto-generate URL_IDs\n",
        "# ---------------------------\n",
        "data = {\n",
        "    \"URL_ID\": [f\"AUTO_{str(i+1).zfill(3)}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "}\n",
        "\n",
        "df_input = pd.DataFrame(data)\n",
        "\n",
        "# ---------------------------\n",
        "# Save Input.xlsx\n",
        "# ---------------------------\n",
        "input_path = \"Input.xlsx\"\n",
        "df_input.to_excel(input_path, index=False)\n",
        "\n",
        "print(\"Input.xlsx created automatically from web URLs\")\n",
        "df_input\n"
      ],
      "metadata": {
        "id": "GfRZydo2lgp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser requests beautifulsoup4 nltk openpyxl"
      ],
      "metadata": {
        "id": "FAHEKAfJmIID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "emXL3-K5mdU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"artificial intelligence\"\n",
        "MAX_URLS = 200\n",
        "\n",
        "rss_url = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "urls = [entry.link for entry in feed.entries[:MAX_URLS]]\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{str(i+1).zfill(3)}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "print(\"Input.xlsx created\")\n"
      ],
      "metadata": {
        "id": "hCRO51ufmlZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"Input.xlsx\"\n",
        "df_input.to_excel(input_path, index=False)\n",
        "\n",
        "print(\"Input.xlsx created automatically from web URLs\")\n",
        "df_input"
      ],
      "metadata": {
        "id": "n5jEaaQFmqNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_text(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        text = \" \".join(p.get_text() for p in paragraphs)\n",
        "\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "ztyQ7OZmm0V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [\n",
        "        w for w in tokens\n",
        "        if w.isalpha() and w not in stop_words\n",
        "    ]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "Fvdj0CYKm3HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "\n",
        "    return max(count, 1)\n",
        "\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n"
      ],
      "metadata": {
        "id": "-a8wyj1Xm5Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    positive_score = sentiment['pos']\n",
        "    negative_score = sentiment['neg']\n",
        "\n",
        "    polarity_score = sentiment['compound']\n",
        "    subjectivity_score = positive_score + negative_score\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    syllable_per_word = (\n",
        "        sum(count_syllables(w) for w in tokens) / word_count\n",
        "        if word_count else 0\n",
        "    )\n",
        "\n",
        "    avg_word_length = (\n",
        "        sum(len(w) for w in tokens) / word_count\n",
        "        if word_count else 0\n",
        "    )\n",
        "\n",
        "    personal_pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    return [\n",
        "        positive_score,\n",
        "        negative_score,\n",
        "        polarity_score,\n",
        "        subjectivity_score,\n",
        "        avg_sentence_length,\n",
        "        percentage_complex_words,\n",
        "        fog_index,\n",
        "        avg_sentence_length,\n",
        "        complex_word_count,\n",
        "        word_count,\n",
        "        syllable_per_word,\n",
        "        personal_pronouns,\n",
        "        avg_word_length\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "nSCFPkMTm7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "8arJzbVFnLR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_rows = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    article_text = extract_article_text(row[\"URL\"])\n",
        "    metrics = analyze_text(article_text)\n",
        "\n",
        "    output_rows.append([row[\"URL_ID\"], row[\"URL\"]] + metrics)\n",
        "\n",
        "columns = [\n",
        "    \"URL_ID\", \"URL\",\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\",\n",
        "    \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
        "    \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\",\n",
        "    \"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "df_output = pd.DataFrame(output_rows, columns=columns)\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "\n",
        "print(\"Output.xlsx generated successfully\")\n"
      ],
      "metadata": {
        "id": "EurzsAjRnC99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "VCKsSVXLnIg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if len(p.strip()) > 0]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "    paragraph_count = len(paragraphs)\n",
        "\n",
        "    # ---- Sentiment ----\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    pos = sentiment['pos']\n",
        "    neg = sentiment['neg']\n",
        "    neu = sentiment['neu']\n",
        "    polarity = sentiment['compound']\n",
        "    intensity = abs(polarity)\n",
        "    emotionality = pos + neg\n",
        "\n",
        "    subjectivity = emotionality / (word_count + 1e-6)\n",
        "\n",
        "    # ---- Complexity ----\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    complex_count = len(complex_words)\n",
        "    pct_complex = complex_count / word_count if word_count else 0\n",
        "\n",
        "    avg_sentence_len = word_count / sentence_count if sentence_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_len + pct_complex)\n",
        "\n",
        "    # ---- Readability ----\n",
        "    flesch = 206.835 - (1.015 * avg_sentence_len) - (84.6 * (syllables / word_count)) if word_count else 0\n",
        "    reading_time = word_count / 225\n",
        "\n",
        "    # ---- Lexical richness ----\n",
        "    unique_words = len(set(tokens))\n",
        "    ttr = unique_words / word_count if word_count else 0\n",
        "\n",
        "    # ---- Style ----\n",
        "    questions = text.count(\"?\")\n",
        "    exclamations = text.count(\"!\")\n",
        "    pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    avg_word_len = sum(len(w) for w in tokens) / word_count if word_count else 0\n",
        "\n",
        "    # ---- Metadata ----\n",
        "    domain = urlparse(url).netloc\n",
        "    length_category = (\n",
        "        \"Short\" if word_count < 500 else\n",
        "        \"Medium\" if word_count <= 1200 else\n",
        "        \"Long\"\n",
        "    )\n",
        "\n",
        "    return [\n",
        "        domain, pos, neg, neu, polarity, intensity, emotionality,\n",
        "        subjectivity, word_count, unique_words, ttr,\n",
        "        sentence_count, avg_sentence_len, paragraph_count,\n",
        "        complex_count, pct_complex, fog_index,\n",
        "        flesch, reading_time, pronouns,\n",
        "        questions, exclamations, avg_word_len, length_category\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "TPbA73HCoDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extended_columns = [\n",
        "    \"URL_ID\", \"URL\", \"DOMAIN\",\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\", \"SENTIMENT INTENSITY\", \"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\n",
        "    \"WORD COUNT\", \"UNIQUE WORD COUNT\", \"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\", \"AVG SENTENCE LENGTH\",\n",
        "    \"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\", \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\", \"READING TIME (MIN)\",\n",
        "    \"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\", \"EXCLAMATION COUNT\",\n",
        "    \"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]"
      ],
      "metadata": {
        "id": "1ik3O0hLoFEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    text = extract_article_text(row[\"URL\"])\n",
        "    metrics = analyze_text_extended(text, row[\"URL\"])\n",
        "    rows.append([row[\"URL_ID\"], row[\"URL\"]] + metrics)\n",
        "\n",
        "df_extended = pd.DataFrame(rows, columns=extended_columns)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "dUODoRtioLe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"Output_Extended.xlsx\"\n",
        "df_input.to_excel(input_path, index=False)\n",
        "\n",
        "print(\"Input.xlsx created automatically from web URLs\")\n",
        "df_input"
      ],
      "metadata": {
        "id": "8_bKt4DBoPGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. INSTALL DEPENDENCIES (COLAB ONLY)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# 2. IMPORTS & NLTK SETUP\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# 3. AUTO-GENERATE INPUT.XLSX FROM WEB\n",
        "# ============================================================\n",
        "QUERY = \"artificial intelligence\"\n",
        "MAX_URLS = 200\n",
        "\n",
        "rss_url = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "urls = [entry.link for entry in feed.entries[:MAX_URLS]]\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{str(i+1).zfill(3)}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SCRAPE ARTICLE TEXT\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        return \" \".join(p.get_text() for p in paragraphs)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# ============================================================\n",
        "# 5. TEXT CLEANING\n",
        "# ============================================================\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "# ============================================================\n",
        "# 6. HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "    return max(count, 1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "# ============================================================\n",
        "# 7. ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    pos = sentiment['pos']\n",
        "    neg = sentiment['neg']\n",
        "    polarity = sentiment['compound']\n",
        "    subjectivity = pos + neg\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_count = len(complex_words)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = complex_count / wc if wc else 0\n",
        "    fog = 0.4 * (avg_sentence_len + pct_complex)\n",
        "\n",
        "    syll_per_word = sum(count_syllables(w) for w in tokens) / wc if wc else 0\n",
        "    avg_word_len = sum(len(w) for w in tokens) / wc if wc else 0\n",
        "    pronouns = count_pronouns(text)\n",
        "\n",
        "    return [\n",
        "        pos, neg, polarity, subjectivity,\n",
        "        avg_sentence_len, pct_complex, fog,\n",
        "        avg_sentence_len, complex_count, wc,\n",
        "        syll_per_word, pronouns, avg_word_len\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 8. EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    pos = sentiment['pos']\n",
        "    neg = sentiment['neg']\n",
        "    neu = sentiment['neu']\n",
        "    polarity = sentiment['compound']\n",
        "\n",
        "    intensity = abs(polarity)\n",
        "    emotionality = pos + neg\n",
        "    subjectivity = emotionality / (wc + 1e-6)\n",
        "\n",
        "    unique_words = len(set(tokens))\n",
        "    ttr = unique_words / wc if wc else 0\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_count = len(complex_words)\n",
        "    pct_complex = complex_count / wc if wc else 0\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    fog = 0.4 * (avg_sentence_len + pct_complex)\n",
        "\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "    flesch = 206.835 - (1.015 * avg_sentence_len) - (84.6 * (syllables / wc)) if wc else 0\n",
        "\n",
        "    reading_time = wc / 225\n",
        "    avg_word_len = sum(len(w) for w in tokens) / wc if wc else 0\n",
        "\n",
        "    pronouns = count_pronouns(text)\n",
        "    questions = text.count(\"?\")\n",
        "    exclamations = text.count(\"!\")\n",
        "\n",
        "    domain = urlparse(url).netloc\n",
        "    length_cat = \"Short\" if wc < 500 else \"Medium\" if wc <= 1200 else \"Long\"\n",
        "\n",
        "    return [\n",
        "        domain, pos, neg, neu, polarity, intensity, emotionality,\n",
        "        subjectivity, wc, unique_words, ttr,\n",
        "        sc, avg_sentence_len, pc,\n",
        "        complex_count, pct_complex, fog,\n",
        "        flesch, reading_time, pronouns,\n",
        "        questions, exclamations, avg_word_len, length_cat\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 9. GENERATE OUTPUT FILES\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    text = extract_article_text(row[\"URL\"])\n",
        "    orig_rows.append([row[\"URL_ID\"], row[\"URL\"]] + analyze_original(text))\n",
        "    ext_rows.append([row[\"URL_ID\"], row[\"URL\"]] + analyze_extended(text, row[\"URL\"]))\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 10. COMPARISON ANALYSIS (PROOF)\n",
        "# ============================================================\n",
        "original_metrics = [\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\"SUBJECTIVITY SCORE\",\"FOG INDEX\"]\n",
        "extended_metrics = [\"NEUTRAL SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\"TYPE TOKEN RATIO\",\"FLESCH READING EASE\"]\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"Original Avg Variance\": [df_output[original_metrics].var().mean()],\n",
        "    \"Extended Avg Variance\": [df_extended[extended_metrics].var().mean()],\n",
        "    \"Original Avg Correlation\": [df_output[original_metrics].corr().abs().mean().mean()],\n",
        "    \"Extended Avg Correlation\": [df_extended[extended_metrics].corr().abs().mean().mean()]\n",
        "})\n",
        "\n",
        "comparison\n"
      ],
      "metadata": {
        "id": "YOTZZiCMpAdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0. INSTALL DEPENDENCIES (Colab only)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# 1. IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# 2. NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# 3. FETCH URLs (at least 100)\n",
        "# ============================================================\n",
        "QUERY = \"artificial intelligence\"\n",
        "MAX_URLS = 200  # Fetch more to ensure at least 100 valid articles\n",
        "\n",
        "rss = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "feed = feedparser.parse(rss)\n",
        "\n",
        "urls = []\n",
        "for entry in feed.entries:\n",
        "    urls.append(entry.link)\n",
        "    if len(urls) >= MAX_URLS:\n",
        "        break\n",
        "\n",
        "# Remove duplicates\n",
        "urls = list(dict.fromkeys(urls))\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "print(f\"Generated Input.xlsx with {len(urls)} URLs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def resolve_url(url):\n",
        "    \"\"\"Follow redirects and get final URL\"\"\"\n",
        "    try:\n",
        "        return requests.get(url, timeout=10, allow_redirects=True).url\n",
        "    except:\n",
        "        return url\n",
        "\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags\"\"\"\n",
        "    try:\n",
        "        url = resolve_url(url)\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "    return max(count, 1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    \"\"\"Return zeros if analysis fails\"\"\"\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# 5. ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],                          # POSITIVE SCORE\n",
        "        s['neg'],                          # NEGATIVE SCORE\n",
        "        s['compound'],                     # POLARITY SCORE\n",
        "        s['pos'] + s['neg'],               # SUBJECTIVITY SCORE\n",
        "        avg_sentence_len,                  # AVG SENTENCE LENGTH\n",
        "        pct_complex,                       # PERCENTAGE OF COMPLEX WORDS\n",
        "        0.4*(avg_sentence_len + pct_complex), # FOG INDEX\n",
        "        avg_sentence_len,                  # AVG NUMBER OF WORDS PER SENTENCE\n",
        "        len(complex_words),                # COMPLEX WORD COUNT\n",
        "        wc,                                # WORD COUNT\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0, # SYLLABLE PER WORD\n",
        "        count_pronouns(text),              # PERSONAL PRONOUNS\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0 # AVG WORD LENGTH\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 6. EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,               # DOMAIN\n",
        "        s['pos'],                           # POSITIVE SCORE\n",
        "        s['neg'],                           # NEGATIVE SCORE\n",
        "        s['neu'],                           # NEUTRAL SCORE\n",
        "        s['compound'],                      # POLARITY SCORE\n",
        "        abs(s['compound']),                 # SENTIMENT INTENSITY\n",
        "        s['pos'] + s['neg'],                # EMOTIONALITY SCORE\n",
        "        (s['pos'] + s['neg'])/(wc + 1e-6), # SUBJECTIVITY SCORE\n",
        "        wc,                                 # WORD COUNT\n",
        "        unique_words,                       # UNIQUE WORD COUNT\n",
        "        unique_words/wc if wc else 0,       # TYPE TOKEN RATIO\n",
        "        sc,                                 # SENTENCE COUNT\n",
        "        avg_sentence_len,                    # AVG SENTENCE LENGTH\n",
        "        pc,                                 # PARAGRAPH COUNT\n",
        "        len(complex_words),                  # COMPLEX WORD COUNT\n",
        "        pct_complex,                         # PERCENTAGE OF COMPLEX WORDS\n",
        "        0.4*(avg_sentence_len + pct_complex), # FOG INDEX\n",
        "        flesch,                             # FLESCH READING EASE\n",
        "        wc/225,                              # READING TIME (MIN)\n",
        "        count_pronouns(text),               # PERSONAL PRONOUNS\n",
        "        text.count(\"?\"),                    # QUESTION COUNT\n",
        "        text.count(\"!\"),                    # EXCLAMATION COUNT\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,  # AVG WORD LENGTH\n",
        "        \"Short\" if wc < 500 else \"Medium\" if wc <= 1200 else \"Long\"  # ARTICLE LENGTH CATEGORY\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 7. COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 8. GENERATE OUTPUTS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for _, r in df_input.iterrows():\n",
        "    text = extract_article_text(r[\"URL\"])\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, r[\"URL\"])\n",
        "    orig_rows.append([r[\"URL_ID\"], r[\"URL\"]] + orig_metrics)\n",
        "    ext_rows.append([r[\"URL_ID\"], r[\"URL\"]] + ext_metrics)\n",
        "    time.sleep(0.2)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# Save XLSX\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "\n",
        "# Save CSV\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 9. VERIFY OUTPUT\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Extended CSV/XLSX generated successfully with at least 100 rows\")\n",
        "print(\"Sample data from extended metrics:\")\n",
        "print(df_extended.head(3))"
      ],
      "metadata": {
        "id": "X5O4jAYrptLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL (COLAB ONLY)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def resolve_url(url):\n",
        "    \"\"\"Follow redirects\"\"\"\n",
        "    try:\n",
        "        return requests.get(url, timeout=10, allow_redirects=True).url\n",
        "    except:\n",
        "        return url\n",
        "\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags\"\"\"\n",
        "    try:\n",
        "        url = resolve_url(url)\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    \"\"\"Return zeros if analysis fails\"\"\"\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FUNCTION TO GET 200 VALID ARTICLES\n",
        "# ============================================================\n",
        "def fetch_valid_articles(topic=\"artificial intelligence\", target_count=200):\n",
        "    urls, texts = [], []\n",
        "    attempt = 0\n",
        "    while len(texts) < target_count:\n",
        "        rss = f\"https://news.google.com/rss/search?q={topic.replace(' ','+')}\"\n",
        "        feed = feedparser.parse(rss)\n",
        "        candidates = [entry.link for entry in feed.entries]\n",
        "        for url in candidates:\n",
        "            if url in urls:  # skip duplicates\n",
        "                continue\n",
        "            text = extract_article_text(url)\n",
        "            if len(text.split()) > 50:  # only accept articles with >50 words\n",
        "                urls.append(url)\n",
        "                texts.append(text)\n",
        "            if len(texts) >= target_count:\n",
        "                break\n",
        "        attempt += 1\n",
        "        if len(texts) < target_count:\n",
        "            topic = \"technology\" if topic==\"artificial intelligence\" else \"science\"\n",
        "            print(f\"Not enough valid articles, changing topic to: {topic}\")\n",
        "        if attempt > 5:\n",
        "            break\n",
        "    return urls[:target_count], texts[:target_count]\n",
        "\n",
        "# ============================================================\n",
        "# 2. FETCH ARTICLES\n",
        "# ============================================================\n",
        "urls, texts = fetch_valid_articles(topic=\"artificial intelligence\", target_count=200)\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url] + ext_metrics)\n",
        "    time.sleep(0.1)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SAVE XLSX + CSV\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5. VERIFY\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Sample of extended metrics:\")\n",
        "print(df_extended.head(3))"
      ],
      "metadata": {
        "id": "FC0mfabIp4CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES (Colab only)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FETCH 200 REAL ARTICLES\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url] + ext_metrics)\n",
        "    time.sleep(0.1)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SAVE XLSX + CSV\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5. VERIFY\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Sample of extended metrics:\")\n",
        "print(df_extended.head(60))"
      ],
      "metadata": {
        "id": "GNMp8physA_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check total rows\n",
        "print(\"Total articles processed:\", len(df_extended))\n",
        "\n",
        "# Show last 5 rows\n",
        "print(df_extended.tail())\n",
        "\n",
        "# Show a summary of extended metrics\n",
        "print(df_extended.describe())"
      ],
      "metadata": {
        "id": "tbdTKCLlsnfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "ubatDqYJstqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES (Colab only)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FETCH 200 REAL ARTICLES\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    # BBC Technology\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    # The Verge\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    # Reuters Technology\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\",\n",
        "    # New York Times Technology\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    # Wired\n",
        "    \"https://www.wired.com/feed/rss\",\n",
        "    # Ars Technica\n",
        "    \"http://feeds.arstechnica.com/arstechnica/index\",\n",
        "    # TechCrunch\n",
        "    \"http://feeds.feedburner.com/TechCrunch/\",\n",
        "    # CNET\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    # Engadget\n",
        "    \"https://www.engadget.com/rss.xml\",\n",
        "    # MIT Technology Review\n",
        "    \"https://www.technologyreview.com/feed/\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url] + ext_metrics)\n",
        "    time.sleep(0.1)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SAVE XLSX + CSV\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5. VERIFY\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Sample of extended metrics:\")\n",
        "print(df_extended.head(3))"
      ],
      "metadata": {
        "id": "1eLcZiuztP2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check total rows\n",
        "print(\"Total articles processed:\", len(df_extended))\n",
        "\n",
        "# Show last 5 rows\n",
        "print(df_extended.tail(2))\n"
      ],
      "metadata": {
        "id": "frIYOkEMt4lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD OUTPUT FILES\n",
        "# ============================================================\n",
        "df_orig = pd.read_excel(\"Output.xlsx\")\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. MERGE ON URL_ID\n",
        "# ============================================================\n",
        "df_compare = pd.merge(df_orig, df_ext, on=\"URL_ID\", suffixes=(\"_orig\", \"_ext\"))\n",
        "\n",
        "# ============================================================\n",
        "# 3. COMPUTE DIFFERENCES\n",
        "# ============================================================\n",
        "metrics_to_compare = [\n",
        "    (\"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\",\n",
        "     \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\",\n",
        "     \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"WORD COUNT\")\n",
        "]\n",
        "\n",
        "# We'll store the differences as new columns\n",
        "for metric in metrics_to_compare[0]:\n",
        "    df_compare[f\"{metric}_diff\"] = df_compare[f\"{metric}_ext\"] - df_compare[f\"{metric}_orig\"]\n",
        "\n",
        "# ============================================================\n",
        "# 4. SUMMARY STATISTICS\n",
        "# ============================================================\n",
        "summary = df_compare[[f\"{m}_diff\" for m in metrics_to_compare[0]]].describe()\n",
        "print(\"Comparison Summary (Extended - Original):\")\n",
        "print(summary)\n",
        "\n",
        "# ============================================================\n",
        "# 5. PLOT COMPARISON\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_compare[0]):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    plt.scatter(df_compare[f\"{metric}_orig\"], df_compare[f\"{metric}_ext\"], alpha=0.6)\n",
        "    plt.plot([df_compare[f\"{metric}_orig\"].min(), df_compare[f\"{metric}_orig\"].max()],\n",
        "             [df_compare[f\"{metric}_orig\"].min(), df_compare[f\"{metric}_orig\"].max()],\n",
        "             color='red', linestyle='--')  # 45-degree line\n",
        "    plt.xlabel(\"Original\")\n",
        "    plt.ylabel(\"Extended\")\n",
        "    plt.title(metric)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 6. OPTIONAL: AVERAGE IMPROVEMENT\n",
        "# ============================================================\n",
        "avg_improvements = df_compare[[f\"{m}_diff\" for m in metrics_to_compare[0]]].mean()\n",
        "print(\"\\nAverage Improvement (Extended - Original):\")\n",
        "print(avg_improvements)"
      ],
      "metadata": {
        "id": "oqyrbxaNuVfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD EXTENDED OUTPUT\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SUMMARY STATISTICS OF EXTENDED-ONLY METRICS\n",
        "# ============================================================\n",
        "extended_only_metrics = [\n",
        "    \"UNIQUE WORD COUNT\", \"TYPE TOKEN RATIO\", \"FLESCH READING EASE\",\n",
        "    \"FOG INDEX\", \"READING TIME (MIN)\", \"QUESTION COUNT\",\n",
        "    \"EXCLAMATION COUNT\", \"COMPLEX WORD COUNT\"\n",
        "]\n",
        "\n",
        "print(\"Summary statistics for extended metrics:\")\n",
        "print(df_ext[extended_only_metrics].describe())\n",
        "\n",
        "# ============================================================\n",
        "# 3. DISTRIBUTION PLOTS\n",
        "# ============================================================\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(18,12))\n",
        "\n",
        "for i, metric in enumerate(extended_only_metrics):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    sns.histplot(df_ext[metric], kde=True, color=\"skyblue\", bins=20)\n",
        "    plt.title(metric)\n",
        "    plt.xlabel(\"\")\n",
        "    plt.ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 4. ARTICLE LENGTH CATEGORY DISTRIBUTION\n",
        "# ============================================================\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"ARTICLE LENGTH CATEGORY\", data=df_ext, palette=\"Set2\")\n",
        "plt.title(\"Distribution of Article Length Categories\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 5. SCATTER PLOTS TO SHOW RELATIONSHIPS\n",
        "# ============================================================\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# TYPE TOKEN RATIO vs FOG Index\n",
        "plt.subplot(1,2,1)\n",
        "sns.scatterplot(x=\"TYPE TOKEN RATIO\", y=\"FOG INDEX\", data=df_ext)\n",
        "plt.title(\"Type Token Ratio vs FOG Index\")\n",
        "plt.xlabel(\"Type Token Ratio\")\n",
        "plt.ylabel(\"FOG Index\")\n",
        "\n",
        "# Flesch Reading Ease vs Reading Time\n",
        "plt.subplot(1,2,2)\n",
        "sns.scatterplot(x=\"FLESCH READING EASE\", y=\"READING TIME (MIN)\", data=df_ext)\n",
        "plt.title(\"Flesch Reading Ease vs Reading Time\")\n",
        "plt.xlabel(\"Flesch Reading Ease\")\n",
        "plt.ylabel(\"Reading Time (minutes)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 6. TOP 10 ARTICLES BY COMPLEX WORD COUNT\n",
        "# ============================================================\n",
        "top_complex = df_ext.sort_values(\"COMPLEX WORD COUNT\", ascending=False).head(10)\n",
        "print(\"Top 10 articles by Complex Word Count:\")\n",
        "print(top_complex[[\"URL_ID\",\"URL\",\"COMPLEX WORD COUNT\",\"FOG INDEX\",\"TYPE TOKEN RATIO\"]])"
      ],
      "metadata": {
        "id": "_YbG2lCuuiLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL (Colab Only)\n",
        "# ============================================================\n",
        "# !pip install wordcloud matplotlib seaborn\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# LOAD EXTENDED OUTPUT\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. GENERATE WORD CLOUD\n",
        "# ============================================================\n",
        "# Combine all article text\n",
        "all_text = \" \".join(df_ext['URL'])  # If you saved the actual text in a column, replace 'URL' with 'TEXT'\n",
        "\n",
        "# If text column is not saved, we need to loop over URLs to extract text\n",
        "# For demonstration, assuming text is in df_ext['ARTICLE_TEXT']\n",
        "try:\n",
        "    all_text = \" \".join(df_ext['ARTICLE_TEXT'])\n",
        "except:\n",
        "    print(\"No text column found; word cloud will use URLs as placeholder\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud = WordCloud(width=1200, height=600, background_color='white', collocations=False).generate(all_text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of All Articles\", fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 2. FIND MOST COMPLEX ARTICLES\n",
        "# ============================================================\n",
        "# Use COMPLEX WORD COUNT or FOG INDEX as complexity metric\n",
        "df_ext['COMPLEXITY_SCORE'] = df_ext['COMPLEX WORD COUNT'] * df_ext['FOG INDEX']\n",
        "\n",
        "# Top 10 most complex articles\n",
        "top_complex = df_ext.sort_values('COMPLEXITY_SCORE', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 Most Complex Articles:\")\n",
        "print(top_complex[['URL_ID','URL','COMPLEX WORD COUNT','FOG INDEX','COMPLEXITY_SCORE']])\n"
      ],
      "metadata": {
        "id": "9LhdOQcJvD7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser requests beautifulsoup4 nltk openpyxl wordcloud seaborn matplotlib"
      ],
      "metadata": {
        "id": "AlL1PYSptqgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "WrqG84uPuofN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# METRIC FUNCTIONS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMN DEFINITIONS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"ARTICLE_TEXT\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# RSS FEEDS (Expanded for 200+ articles)\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\",\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    \"https://www.wired.com/feed/rss\",\n",
        "    \"http://feeds.arstechnica.com/arstechnica/index\",\n",
        "    \"http://feeds.feedburner.com/TechCrunch/\",\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    \"https://www.engadget.com/rss.xml\",\n",
        "    \"https://www.technologyreview.com/feed/\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url, text] + ext_metrics)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# SAVE OUTPUT FILES\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 1. GENERATE WORD CLOUD\n",
        "# ============================================================\n",
        "all_text = \" \".join(df_extended['ARTICLE_TEXT'])\n",
        "#wordcloud = WordCloud(width=1200, height=600, background_color='white', collocations=False).generate(all_text)\n",
        "\n",
        "#plt.figure(figsize=(15,7))\n",
        "#plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#plt.axis('off')\n",
        "#plt.title(\"Word Cloud of All Articles\", fontsize=20)\n",
        "#plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 2. MOST COMPLEX ARTICLES\n",
        "# ============================================================\n",
        "df_extended['COMPLEXITY_SCORE'] = df_extended['COMPLEX WORD COUNT'] * df_extended['FOG INDEX']\n",
        "top_complex = df_extended.sort_values('COMPLEXITY_SCORE', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 Most Complex Articles:\")\n",
        "print(top_complex[['URL_ID','URL','COMPLEX WORD COUNT','FOG INDEX','COMPLEXITY_SCORE']])"
      ],
      "metadata": {
        "id": "VITOZELhx2Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL TRANSFORMERS\n",
        "# ============================================================\n",
        "# !pip install transformers torch\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# LOAD EXTENDED DATA\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZE HUGGING FACE SENTIMENT ANALYZER\n",
        "# ============================================================\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. GET MODEL PREDICTIONS\n",
        "# ============================================================\n",
        "pred_labels = []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment with Model\"):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        pred_labels.append(\"NEUTRAL\")  # handle empty text\n",
        "    else:\n",
        "        res = sentiment_model(text[:512])[0]  # limit to first 512 tokens for speed\n",
        "        pred_labels.append(res['label'].upper())  # \"POSITIVE\" or \"NEGATIVE\"\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = pred_labels\n",
        "\n",
        "# ============================================================\n",
        "# 2. VADER SENTIMENT LABEL\n",
        "# ============================================================\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "vader_labels = []\n",
        "\n",
        "for text in df_ext['ARTICLE_TEXT']:\n",
        "    s = sia.polarity_scores(str(text))\n",
        "    vader_labels.append(\"POSITIVE\" if s['compound'] >= 0 else \"NEGATIVE\")\n",
        "\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "\n",
        "# ============================================================\n",
        "# 3. COMPUTE ACCURACY\n",
        "# ============================================================\n",
        "accuracy = (df_ext['MODEL_SENTIMENT'] == df_ext['VADER_SENTIMENT']).mean()\n",
        "print(f\"Accuracy of VADER sentiment vs pre-trained model: {accuracy*100:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. OPTIONAL: SHOW MISMATCHES\n",
        "# ============================================================\n",
        "mismatches = df_ext[df_ext['MODEL_SENTIMENT'] != df_ext['VADER_SENTIMENT']]\n",
        "print(f\"\\nNumber of mismatches: {len(mismatches)}\")\n",
        "print(mismatches[['URL_ID','URL','VADER_SENTIMENT','MODEL_SENTIMENT']])\n"
      ],
      "metadata": {
        "id": "0Vpj_eOJygU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_ext['MODEL_SENTIMENT'], df_ext['VADER_SENTIMENT']))\n"
      ],
      "metadata": {
        "id": "AnqpyaQczBSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES (if not already installed)\n",
        "# ============================================================\n",
        "# !pip install transformers torch pandas nltk openpyxl tqdm\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# ============================================================\n",
        "# LOAD EXTENDED DATA\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZE SENTIMENT MODELS\n",
        "# ============================================================\n",
        "# Hugging Face pre-trained sentiment model\n",
        "model_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# 1. MODEL PREDICTIONS\n",
        "# ============================================================\n",
        "model_labels = []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment with Model\"):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        model_labels.append(\"NEUTRAL\")\n",
        "    else:\n",
        "        res = model_analyzer(text[:512])[0]  # limit for speed\n",
        "        model_labels.append(res['label'].upper())\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = model_labels\n",
        "\n",
        "# ============================================================\n",
        "# 2. VADER STRONG SENTIMENT LABELS\n",
        "# ============================================================\n",
        "vader_labels = []\n",
        "\n",
        "for text in df_ext['ARTICLE_TEXT']:\n",
        "    s = sia.polarity_scores(str(text))\n",
        "    compound = s['compound']\n",
        "    if compound > 0.05:\n",
        "        vader_labels.append(\"POSITIVE\")\n",
        "    elif compound < -0.05:\n",
        "        vader_labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "\n",
        "# ============================================================\n",
        "# 3. ACCURACY AGAINST MODEL\n",
        "# ============================================================\n",
        "accuracy = (df_ext['MODEL_SENTIMENT'] == df_ext['VADER_SENTIMENT']).mean()\n",
        "print(f\"Accuracy of VADER (strong sentiment) vs pre-trained model: {accuracy*100:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. OPTIONAL: SHOW MISMATCHES\n",
        "# ============================================================\n",
        "mismatches = df_ext[df_ext['MODEL_SENTIMENT'] != df_ext['VADER_SENTIMENT']]\n",
        "print(f\"\\nNumber of mismatches: {len(mismatches)}\")\n",
        "print(mismatches[['URL_ID','URL','VADER_SENTIMENT','MODEL_SENTIMENT']])"
      ],
      "metadata": {
        "id": "KYYh7WQqzPRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_ext['MODEL_SENTIMENT'], df_ext['VADER_SENTIMENT']))"
      ],
      "metadata": {
        "id": "2nJAJ0kGziSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. GET MODEL PREDICTIONS AND VADER SCORES\n",
        "# ============================================================\n",
        "model_probs, model_labels, vader_compound, vader_labels = [], [], [], []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment\"):\n",
        "    if not isinstance(text, str) or len(text.strip())==0:\n",
        "        model_probs.append(0.5)\n",
        "        model_labels.append(\"NEUTRAL\")\n",
        "        vader_compound.append(0)\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "        continue\n",
        "\n",
        "    # Hugging Face model prediction\n",
        "    res = model_analyzer(text[:512])[0]\n",
        "    model_labels.append(res['label'].upper())\n",
        "    model_probs.append(res['score'] if res['label'].upper()=='POSITIVE' else 1-res['score'])\n",
        "\n",
        "    # VADER\n",
        "    s = sia.polarity_scores(text)\n",
        "    compound = s['compound']\n",
        "    vader_compound.append(compound)\n",
        "    if compound > 0.05:\n",
        "        vader_labels.append(\"POSITIVE\")\n",
        "    elif compound < -0.05:\n",
        "        vader_labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = model_labels\n",
        "df_ext['MODEL_PROB'] = model_probs\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "df_ext['VADER_COMPOUND'] = vader_compound\n",
        "\n",
        "# ============================================================\n",
        "# 2. SCATTER PLOT: VADER compound vs MODEL probability\n",
        "# ============================================================\n",
        "#plt.figure(figsize=(10,6))\n",
        "#sns.scatterplot(x='VADER_COMPOUND', y='MODEL_PROB', hue='MODEL_SENTIMENT', data=df_ext, alpha=0.7)\n",
        "#plt.axvline(0.05, color='red', linestyle='--', label='VADER POS threshold')\n",
        "#plt.axvline(-0.05, color='blue', linestyle='--', label='VADER NEG threshold')\n",
        "#plt.xlabel(\"VADER Compound Score\")\n",
        "#plt.ylabel(\"ML Model Positive Probability\")\n",
        "#plt.title(\"VADER Compound Score vs ML Model Sentiment Probability\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 3. MISMATCHES\n",
        "# ============================================================\n",
        "mismatches = df_ext[df_ext['VADER_SENTIMENT'] != df_ext['MODEL_SENTIMENT']]\n",
        "print(f\"Total mismatches: {len(mismatches)}\")\n",
        "display_cols = ['URL_ID','URL','VADER_SENTIMENT','VADER_COMPOUND','MODEL_SENTIMENT','MODEL_PROB']\n",
        "print(mismatches[display_cols].head(10))  # show top 10 mismatches\n",
        "\n",
        "# ============================================================\n",
        "# 4. TOP 5 MOST EXTREME MISMATCHES\n",
        "# ============================================================\n",
        "# Extreme mismatch = VADER strongly opposite to model\n",
        "mismatches['EXTREME_DIFF'] = abs(mismatches['VADER_COMPOUND'] - (mismatches['MODEL_PROB']*2-1))\n",
        "top_extreme = mismatches.sort_values('EXTREME_DIFF', ascending=False).head(5)\n",
        "print(\"\\nTop 5 Extreme Mismatches:\")\n",
        "print(top_extreme[display_cols])"
      ],
      "metadata": {
        "id": "BnwaZiXTzyfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IoiYJaI0xrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}