{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfs4QYfIUCdnlmNsIA2Ocg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e85cd793fed44a5a28aa03df4df2726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15cb254f81e347c19f66937b634fc8bb",
              "IPY_MODEL_f180f217bc434cb8a4c2598c34face21",
              "IPY_MODEL_0580e1e83109424a8baa1e54a31af2ae"
            ],
            "layout": "IPY_MODEL_2080110b2705459d8f24a17b47568b70"
          }
        },
        "15cb254f81e347c19f66937b634fc8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d9f0a7c0bbf4ef0955913b08c2095ad",
            "placeholder": "​",
            "style": "IPY_MODEL_03295715d9204a0482ae003ae618a520",
            "value": "config.json: 100%"
          }
        },
        "f180f217bc434cb8a4c2598c34face21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e91c9b4b1634f1583c2d207ce0a4fd9",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9764364228b0432aa9f2997bb4cf782a",
            "value": 629
          }
        },
        "0580e1e83109424a8baa1e54a31af2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fadb21e7053e4d008a14b70a9ef7001f",
            "placeholder": "​",
            "style": "IPY_MODEL_e2c746b326254018bceaebe2ec4dab42",
            "value": " 629/629 [00:00&lt;00:00, 11.1kB/s]"
          }
        },
        "2080110b2705459d8f24a17b47568b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9f0a7c0bbf4ef0955913b08c2095ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03295715d9204a0482ae003ae618a520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e91c9b4b1634f1583c2d207ce0a4fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9764364228b0432aa9f2997bb4cf782a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fadb21e7053e4d008a14b70a9ef7001f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2c746b326254018bceaebe2ec4dab42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3ec46ed81dd496794e5d68b4dc07fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18b42ef4707b463e8037c606c7931305",
              "IPY_MODEL_a8f1e3f189174262ac1e2a848375ab96",
              "IPY_MODEL_9a9d0b6dd0c44225a3661029104a6868"
            ],
            "layout": "IPY_MODEL_f6b73e7ce62a4621a3b1ee9e45bda8a1"
          }
        },
        "18b42ef4707b463e8037c606c7931305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_579b7dea4fa442cd9a5208e2e5e30275",
            "placeholder": "​",
            "style": "IPY_MODEL_5f5e546a7c63473a94c73c9bd5380fcd",
            "value": "model.safetensors: 100%"
          }
        },
        "a8f1e3f189174262ac1e2a848375ab96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c735ae7d49fa4c40add412f72ae7ade6",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21a8440e7e564bf5ad5b3ac31c42cb48",
            "value": 267832558
          }
        },
        "9a9d0b6dd0c44225a3661029104a6868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29060de86f204bd2967fe6b4b642368a",
            "placeholder": "​",
            "style": "IPY_MODEL_b37cc9aec94a40ce94f257708d747b85",
            "value": " 268M/268M [00:02&lt;00:00, 137MB/s]"
          }
        },
        "f6b73e7ce62a4621a3b1ee9e45bda8a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "579b7dea4fa442cd9a5208e2e5e30275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f5e546a7c63473a94c73c9bd5380fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c735ae7d49fa4c40add412f72ae7ade6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21a8440e7e564bf5ad5b3ac31c42cb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29060de86f204bd2967fe6b4b642368a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b37cc9aec94a40ce94f257708d747b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cb5ff851d664faaa3dc71216c3e9f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b80dd210d42b4034a63c8491b5f4a4b2",
              "IPY_MODEL_7ef0c797894440b2a5ef5e9c82a743e3",
              "IPY_MODEL_efe079ccf3c9437fbc9692b47de9b47e"
            ],
            "layout": "IPY_MODEL_31693b7478f14e9b996657fa35459390"
          }
        },
        "b80dd210d42b4034a63c8491b5f4a4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a71bb62a2ed4810babe3a9e64c40b5f",
            "placeholder": "​",
            "style": "IPY_MODEL_0877926f33a4427186deb10274a516f0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7ef0c797894440b2a5ef5e9c82a743e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c7f59cbde7a41ca88a54d21070d0017",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40a7837c1eb949d6a1417117f013a4d4",
            "value": 48
          }
        },
        "efe079ccf3c9437fbc9692b47de9b47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bdf56a5e3d54b6e9719660206760edc",
            "placeholder": "​",
            "style": "IPY_MODEL_5267b92b976a4ddf8de8a8d72fe37eda",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.24kB/s]"
          }
        },
        "31693b7478f14e9b996657fa35459390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a71bb62a2ed4810babe3a9e64c40b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0877926f33a4427186deb10274a516f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c7f59cbde7a41ca88a54d21070d0017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a7837c1eb949d6a1417117f013a4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bdf56a5e3d54b6e9719660206760edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5267b92b976a4ddf8de8a8d72fe37eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3f51dbbfb304a93ae60ae8d0bb08533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_646b6c0050794e76be74537ef6e49596",
              "IPY_MODEL_5905a36bcccf44ff931d042c43c38f50",
              "IPY_MODEL_bebcfe0bdb2a43e2a2bfaefe38d85e17"
            ],
            "layout": "IPY_MODEL_2422c143e823476388b4e342b63e8a04"
          }
        },
        "646b6c0050794e76be74537ef6e49596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed7d895e9f24020a7380c6f3e1c7ff0",
            "placeholder": "​",
            "style": "IPY_MODEL_c693c9189b174639a77d7ed8b541bca1",
            "value": "vocab.txt: 100%"
          }
        },
        "5905a36bcccf44ff931d042c43c38f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a8c96bf6e5c4068881c7cc8d3051b80",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d419aec61bb8457d990565fb91af7720",
            "value": 231508
          }
        },
        "bebcfe0bdb2a43e2a2bfaefe38d85e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99875ffeb5c840edab9d913289cf8b19",
            "placeholder": "​",
            "style": "IPY_MODEL_3d116751bed7453ba4ae335f924d9961",
            "value": " 232k/232k [00:00&lt;00:00, 2.49MB/s]"
          }
        },
        "2422c143e823476388b4e342b63e8a04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ed7d895e9f24020a7380c6f3e1c7ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c693c9189b174639a77d7ed8b541bca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8c96bf6e5c4068881c7cc8d3051b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d419aec61bb8457d990565fb91af7720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99875ffeb5c840edab9d913289cf8b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d116751bed7453ba4ae335f924d9961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASantra-star/Scrapping_Sentiment_Analysis/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV65PkLS3Vpr"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 requests nltk pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "sIc_ePpi3YOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_url = \"https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/export?format=xlsx\"\n",
        "input_path = \"/content/Input.xlsx\"\n",
        "\n",
        "r = requests.get(input_url)\n",
        "with open(input_path, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "df_input = pd.read_excel(input_path)\n",
        "df_input"
      ],
      "metadata": {
        "id": "X3pQoem33q64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_words_from_files(file_list):\n",
        "    words = set()\n",
        "    for file in file_list:\n",
        "        with open(file, \"r\", encoding=\"latin-1\") as f:\n",
        "            words.update(word.strip().lower() for word in f)\n",
        "    return words"
      ],
      "metadata": {
        "id": "TyBscbPx3rs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "Uxh5kDlp4s1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# Folder 1\n",
        "folder1_id = \"1rd7YdoX8tED9mujc0c-6evJU4y7LFc_R\"\n",
        "gdown.download_folder(id=folder1_id, output=\"StopWords\", quiet=False)\n",
        "\n",
        "# Folder 2\n",
        "folder2_id = \"1YRcVlJO3ZaC78iTC6JcunfZl7Fz4AL8v\"\n",
        "gdown.download_folder(id=folder2_id, output=\"MasterDictionary\", quiet=False)\n"
      ],
      "metadata": {
        "id": "Ur-UpBi83zK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_files = [\n",
        "    '/content/StopWords/StopWords_Auditor.txt',\n",
        "    '/content/StopWords/StopWords_Currencies.txt',\n",
        "    '/content/StopWords/StopWords_DatesandNumbers.txt',\n",
        "    '/content/StopWords/StopWords_Generic.txt',\n",
        "    '/content/StopWords/StopWords_GenericLong.txt',\n",
        "    '/content/StopWords/StopWords_Geographic.txt',\n",
        "    '/content/StopWords/StopWords_Names.txt'\n",
        "]\n",
        "\n",
        "stop_words = load_words_from_files(stopword_files)"
      ],
      "metadata": {
        "id": "bBDaRQcN31rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = load_words_from_files(['/content/MasterDictionary/positive-words.txt'])\n",
        "negative_words = load_words_from_files(['/content/MasterDictionary/negative-words.txt'])"
      ],
      "metadata": {
        "id": "RM0QrjwI5GTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        article = soup.find('article')\n",
        "        if article:\n",
        "            return article.get_text(separator=' ')\n",
        "        else:\n",
        "            return soup.get_text()\n",
        "    except:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "p0nHp2xb5I9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "pGtIWWhX5MsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "\n",
        "    return max(count, 1)"
      ],
      "metadata": {
        "id": "M6ydWpE65PYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_text(text)\n",
        "\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    positive_score = sum(1 for w in tokens if w in positive_words)\n",
        "    negative_score = sum(1 for w in tokens if w in negative_words)\n",
        "\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    syllables_per_word = sum(count_syllables(w) for w in tokens) / word_count if word_count else 0\n",
        "    avg_word_length = sum(len(w) for w in tokens) / word_count if word_count else 0\n",
        "\n",
        "    pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    return [\n",
        "        positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "        avg_sentence_length, percentage_complex_words, fog_index,\n",
        "        avg_sentence_length, complex_word_count, word_count,\n",
        "        syllables_per_word, pronouns, avg_word_length\n",
        "    ]"
      ],
      "metadata": {
        "id": "HF9mA2I15R59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "NDP3XJGm5svl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_data = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    text = extract_article_text(row['URL'])\n",
        "    metrics = analyze_text(text)\n",
        "\n",
        "    output_data.append([row['URL_ID'], row['URL']] + metrics)"
      ],
      "metadata": {
        "id": "VDrucw6E5Xqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "df_output = pd.DataFrame(output_data, columns=columns)\n",
        "\n",
        "output_path = \"/content/Output.xlsx\"\n",
        "df_output.to_excel(output_path, index=False)\n",
        "\n",
        "df_output\n"
      ],
      "metadata": {
        "id": "jQTtxrE95asX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser openpyxl"
      ],
      "metadata": {
        "id": "w-8C34ad7PhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "QUERY = \"financial markets\"\n",
        "MAX_URLS = 400\n",
        "\n",
        "# Google News RSS feed\n",
        "rss_url = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Fetch URLs from web\n",
        "# ---------------------------\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "urls = []\n",
        "for entry in feed.entries[:MAX_URLS]:\n",
        "    urls.append(entry.link)\n",
        "\n",
        "# ---------------------------\n",
        "# Auto-generate URL_IDs\n",
        "# ---------------------------\n",
        "data = {\n",
        "    \"URL_ID\": [f\"AUTO_{str(i+1).zfill(3)}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "}\n",
        "\n",
        "df_input = pd.DataFrame(data)\n",
        "\n",
        "# ---------------------------\n",
        "# Save Input.xlsx\n",
        "# ---------------------------\n",
        "input_path = \"Input.xlsx\"\n",
        "df_input.to_excel(input_path, index=False)\n",
        "\n",
        "print(\"Input.xlsx created automatically from web URLs\")\n",
        "df_input\n"
      ],
      "metadata": {
        "id": "GfRZydo2lgp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser requests beautifulsoup4 nltk openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAHEKAfJmIID",
        "outputId": "4acd8468-1d16-4158-dd68-416a207910b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=fd49737abcfed6943745ad6a312de1402f19764b3068d77e1d701293d51ef320\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emXL3-K5mdU_",
        "outputId": "1f146a3e-f1bf-4762-a1bb-9ae3bab08115"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"artificial intelligence\"\n",
        "MAX_URLS = 200\n",
        "\n",
        "rss_url = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "urls = [entry.link for entry in feed.entries[:MAX_URLS]]\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{str(i+1).zfill(3)}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "print(\"Input.xlsx created\")\n"
      ],
      "metadata": {
        "id": "hCRO51ufmlZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"Input.xlsx\"\n",
        "df_input.to_excel(input_path, index=False)\n",
        "\n",
        "print(\"Input.xlsx created automatically from web URLs\")\n",
        "df_input"
      ],
      "metadata": {
        "id": "n5jEaaQFmqNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_text(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        text = \" \".join(p.get_text() for p in paragraphs)\n",
        "\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "ztyQ7OZmm0V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [\n",
        "        w for w in tokens\n",
        "        if w.isalpha() and w not in stop_words\n",
        "    ]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "Fvdj0CYKm3HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "\n",
        "    return max(count, 1)\n",
        "\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n"
      ],
      "metadata": {
        "id": "-a8wyj1Xm5Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    positive_score = sentiment['pos']\n",
        "    negative_score = sentiment['neg']\n",
        "\n",
        "    polarity_score = sentiment['compound']\n",
        "    subjectivity_score = positive_score + negative_score\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    syllable_per_word = (\n",
        "        sum(count_syllables(w) for w in tokens) / word_count\n",
        "        if word_count else 0\n",
        "    )\n",
        "\n",
        "    avg_word_length = (\n",
        "        sum(len(w) for w in tokens) / word_count\n",
        "        if word_count else 0\n",
        "    )\n",
        "\n",
        "    personal_pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    return [\n",
        "        positive_score,\n",
        "        negative_score,\n",
        "        polarity_score,\n",
        "        subjectivity_score,\n",
        "        avg_sentence_length,\n",
        "        percentage_complex_words,\n",
        "        fog_index,\n",
        "        avg_sentence_length,\n",
        "        complex_word_count,\n",
        "        word_count,\n",
        "        syllable_per_word,\n",
        "        personal_pronouns,\n",
        "        avg_word_length\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "nSCFPkMTm7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "8arJzbVFnLR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_rows = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    article_text = extract_article_text(row[\"URL\"])\n",
        "    metrics = analyze_text(article_text)\n",
        "\n",
        "    output_rows.append([row[\"URL_ID\"], row[\"URL\"]] + metrics)\n",
        "\n",
        "columns = [\n",
        "    \"URL_ID\", \"URL\",\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\",\n",
        "    \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
        "    \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\",\n",
        "    \"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "df_output = pd.DataFrame(output_rows, columns=columns)\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "\n",
        "print(\"Output.xlsx generated successfully\")\n"
      ],
      "metadata": {
        "id": "EurzsAjRnC99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "VCKsSVXLnIg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if len(p.strip()) > 0]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "    paragraph_count = len(paragraphs)\n",
        "\n",
        "    # ---- Sentiment ----\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    pos = sentiment['pos']\n",
        "    neg = sentiment['neg']\n",
        "    neu = sentiment['neu']\n",
        "    polarity = sentiment['compound']\n",
        "    intensity = abs(polarity)\n",
        "    emotionality = pos + neg\n",
        "\n",
        "    subjectivity = emotionality / (word_count + 1e-6)\n",
        "\n",
        "    # ---- Complexity ----\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    complex_count = len(complex_words)\n",
        "    pct_complex = complex_count / word_count if word_count else 0\n",
        "\n",
        "    avg_sentence_len = word_count / sentence_count if sentence_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_len + pct_complex)\n",
        "\n",
        "    # ---- Readability ----\n",
        "    flesch = 206.835 - (1.015 * avg_sentence_len) - (84.6 * (syllables / word_count)) if word_count else 0\n",
        "    reading_time = word_count / 225\n",
        "\n",
        "    # ---- Lexical richness ----\n",
        "    unique_words = len(set(tokens))\n",
        "    ttr = unique_words / word_count if word_count else 0\n",
        "\n",
        "    # ---- Style ----\n",
        "    questions = text.count(\"?\")\n",
        "    exclamations = text.count(\"!\")\n",
        "    pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    avg_word_len = sum(len(w) for w in tokens) / word_count if word_count else 0\n",
        "\n",
        "    # ---- Metadata ----\n",
        "    domain = urlparse(url).netloc\n",
        "    length_category = (\n",
        "        \"Short\" if word_count < 500 else\n",
        "        \"Medium\" if word_count <= 1200 else\n",
        "        \"Long\"\n",
        "    )\n",
        "\n",
        "    return [\n",
        "        domain, pos, neg, neu, polarity, intensity, emotionality,\n",
        "        subjectivity, word_count, unique_words, ttr,\n",
        "        sentence_count, avg_sentence_len, paragraph_count,\n",
        "        complex_count, pct_complex, fog_index,\n",
        "        flesch, reading_time, pronouns,\n",
        "        questions, exclamations, avg_word_len, length_category\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "TPbA73HCoDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extended_columns = [\n",
        "    \"URL_ID\", \"URL\", \"DOMAIN\",\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\", \"SENTIMENT INTENSITY\", \"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\n",
        "    \"WORD COUNT\", \"UNIQUE WORD COUNT\", \"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\", \"AVG SENTENCE LENGTH\",\n",
        "    \"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\", \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\", \"READING TIME (MIN)\",\n",
        "    \"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\", \"EXCLAMATION COUNT\",\n",
        "    \"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]"
      ],
      "metadata": {
        "id": "1ik3O0hLoFEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    text = extract_article_text(row[\"URL\"])\n",
        "    metrics = analyze_text_extended(text, row[\"URL\"])\n",
        "    rows.append([row[\"URL_ID\"], row[\"URL\"]] + metrics)\n",
        "\n",
        "df_extended = pd.DataFrame(rows, columns=extended_columns)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "dUODoRtioLe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"Output_Extended.xlsx\"\n",
        "df_input.to_excel(input_path, index=False)\n",
        "\n",
        "print(\"Input.xlsx created automatically from web URLs\")\n",
        "df_input"
      ],
      "metadata": {
        "id": "8_bKt4DBoPGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. INSTALL DEPENDENCIES (COLAB ONLY)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# 2. IMPORTS & NLTK SETUP\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# 3. AUTO-GENERATE INPUT.XLSX FROM WEB\n",
        "# ============================================================\n",
        "QUERY = \"artificial intelligence\"\n",
        "MAX_URLS = 200\n",
        "\n",
        "rss_url = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "urls = [entry.link for entry in feed.entries[:MAX_URLS]]\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{str(i+1).zfill(3)}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SCRAPE ARTICLE TEXT\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        return \" \".join(p.get_text() for p in paragraphs)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# ============================================================\n",
        "# 5. TEXT CLEANING\n",
        "# ============================================================\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "# ============================================================\n",
        "# 6. HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "    return max(count, 1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "# ============================================================\n",
        "# 7. ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    pos = sentiment['pos']\n",
        "    neg = sentiment['neg']\n",
        "    polarity = sentiment['compound']\n",
        "    subjectivity = pos + neg\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_count = len(complex_words)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = complex_count / wc if wc else 0\n",
        "    fog = 0.4 * (avg_sentence_len + pct_complex)\n",
        "\n",
        "    syll_per_word = sum(count_syllables(w) for w in tokens) / wc if wc else 0\n",
        "    avg_word_len = sum(len(w) for w in tokens) / wc if wc else 0\n",
        "    pronouns = count_pronouns(text)\n",
        "\n",
        "    return [\n",
        "        pos, neg, polarity, subjectivity,\n",
        "        avg_sentence_len, pct_complex, fog,\n",
        "        avg_sentence_len, complex_count, wc,\n",
        "        syll_per_word, pronouns, avg_word_len\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 8. EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    pos = sentiment['pos']\n",
        "    neg = sentiment['neg']\n",
        "    neu = sentiment['neu']\n",
        "    polarity = sentiment['compound']\n",
        "\n",
        "    intensity = abs(polarity)\n",
        "    emotionality = pos + neg\n",
        "    subjectivity = emotionality / (wc + 1e-6)\n",
        "\n",
        "    unique_words = len(set(tokens))\n",
        "    ttr = unique_words / wc if wc else 0\n",
        "\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    complex_count = len(complex_words)\n",
        "    pct_complex = complex_count / wc if wc else 0\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    fog = 0.4 * (avg_sentence_len + pct_complex)\n",
        "\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "    flesch = 206.835 - (1.015 * avg_sentence_len) - (84.6 * (syllables / wc)) if wc else 0\n",
        "\n",
        "    reading_time = wc / 225\n",
        "    avg_word_len = sum(len(w) for w in tokens) / wc if wc else 0\n",
        "\n",
        "    pronouns = count_pronouns(text)\n",
        "    questions = text.count(\"?\")\n",
        "    exclamations = text.count(\"!\")\n",
        "\n",
        "    domain = urlparse(url).netloc\n",
        "    length_cat = \"Short\" if wc < 500 else \"Medium\" if wc <= 1200 else \"Long\"\n",
        "\n",
        "    return [\n",
        "        domain, pos, neg, neu, polarity, intensity, emotionality,\n",
        "        subjectivity, wc, unique_words, ttr,\n",
        "        sc, avg_sentence_len, pc,\n",
        "        complex_count, pct_complex, fog,\n",
        "        flesch, reading_time, pronouns,\n",
        "        questions, exclamations, avg_word_len, length_cat\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 9. GENERATE OUTPUT FILES\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    text = extract_article_text(row[\"URL\"])\n",
        "    orig_rows.append([row[\"URL_ID\"], row[\"URL\"]] + analyze_original(text))\n",
        "    ext_rows.append([row[\"URL_ID\"], row[\"URL\"]] + analyze_extended(text, row[\"URL\"]))\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 10. COMPARISON ANALYSIS (PROOF)\n",
        "# ============================================================\n",
        "original_metrics = [\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\"SUBJECTIVITY SCORE\",\"FOG INDEX\"]\n",
        "extended_metrics = [\"NEUTRAL SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\"TYPE TOKEN RATIO\",\"FLESCH READING EASE\"]\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"Original Avg Variance\": [df_output[original_metrics].var().mean()],\n",
        "    \"Extended Avg Variance\": [df_extended[extended_metrics].var().mean()],\n",
        "    \"Original Avg Correlation\": [df_output[original_metrics].corr().abs().mean().mean()],\n",
        "    \"Extended Avg Correlation\": [df_extended[extended_metrics].corr().abs().mean().mean()]\n",
        "})\n",
        "\n",
        "comparison\n"
      ],
      "metadata": {
        "id": "YOTZZiCMpAdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0. INSTALL DEPENDENCIES (Colab only)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# 1. IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# 2. NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# 3. FETCH URLs (at least 100)\n",
        "# ============================================================\n",
        "QUERY = \"artificial intelligence\"\n",
        "MAX_URLS = 200  # Fetch more to ensure at least 100 valid articles\n",
        "\n",
        "rss = f\"https://news.google.com/rss/search?q={QUERY.replace(' ', '+')}\"\n",
        "feed = feedparser.parse(rss)\n",
        "\n",
        "urls = []\n",
        "for entry in feed.entries:\n",
        "    urls.append(entry.link)\n",
        "    if len(urls) >= MAX_URLS:\n",
        "        break\n",
        "\n",
        "# Remove duplicates\n",
        "urls = list(dict.fromkeys(urls))\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "print(f\"Generated Input.xlsx with {len(urls)} URLs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def resolve_url(url):\n",
        "    \"\"\"Follow redirects and get final URL\"\"\"\n",
        "    try:\n",
        "        return requests.get(url, timeout=10, allow_redirects=True).url\n",
        "    except:\n",
        "        return url\n",
        "\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags\"\"\"\n",
        "    try:\n",
        "        url = resolve_url(url)\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\", \"ed\")):\n",
        "        count -= 1\n",
        "    return max(count, 1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    \"\"\"Return zeros if analysis fails\"\"\"\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# 5. ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],                          # POSITIVE SCORE\n",
        "        s['neg'],                          # NEGATIVE SCORE\n",
        "        s['compound'],                     # POLARITY SCORE\n",
        "        s['pos'] + s['neg'],               # SUBJECTIVITY SCORE\n",
        "        avg_sentence_len,                  # AVG SENTENCE LENGTH\n",
        "        pct_complex,                       # PERCENTAGE OF COMPLEX WORDS\n",
        "        0.4*(avg_sentence_len + pct_complex), # FOG INDEX\n",
        "        avg_sentence_len,                  # AVG NUMBER OF WORDS PER SENTENCE\n",
        "        len(complex_words),                # COMPLEX WORD COUNT\n",
        "        wc,                                # WORD COUNT\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0, # SYLLABLE PER WORD\n",
        "        count_pronouns(text),              # PERSONAL PRONOUNS\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0 # AVG WORD LENGTH\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 6. EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,               # DOMAIN\n",
        "        s['pos'],                           # POSITIVE SCORE\n",
        "        s['neg'],                           # NEGATIVE SCORE\n",
        "        s['neu'],                           # NEUTRAL SCORE\n",
        "        s['compound'],                      # POLARITY SCORE\n",
        "        abs(s['compound']),                 # SENTIMENT INTENSITY\n",
        "        s['pos'] + s['neg'],                # EMOTIONALITY SCORE\n",
        "        (s['pos'] + s['neg'])/(wc + 1e-6), # SUBJECTIVITY SCORE\n",
        "        wc,                                 # WORD COUNT\n",
        "        unique_words,                       # UNIQUE WORD COUNT\n",
        "        unique_words/wc if wc else 0,       # TYPE TOKEN RATIO\n",
        "        sc,                                 # SENTENCE COUNT\n",
        "        avg_sentence_len,                    # AVG SENTENCE LENGTH\n",
        "        pc,                                 # PARAGRAPH COUNT\n",
        "        len(complex_words),                  # COMPLEX WORD COUNT\n",
        "        pct_complex,                         # PERCENTAGE OF COMPLEX WORDS\n",
        "        0.4*(avg_sentence_len + pct_complex), # FOG INDEX\n",
        "        flesch,                             # FLESCH READING EASE\n",
        "        wc/225,                              # READING TIME (MIN)\n",
        "        count_pronouns(text),               # PERSONAL PRONOUNS\n",
        "        text.count(\"?\"),                    # QUESTION COUNT\n",
        "        text.count(\"!\"),                    # EXCLAMATION COUNT\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,  # AVG WORD LENGTH\n",
        "        \"Short\" if wc < 500 else \"Medium\" if wc <= 1200 else \"Long\"  # ARTICLE LENGTH CATEGORY\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# 7. COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 8. GENERATE OUTPUTS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for _, r in df_input.iterrows():\n",
        "    text = extract_article_text(r[\"URL\"])\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, r[\"URL\"])\n",
        "    orig_rows.append([r[\"URL_ID\"], r[\"URL\"]] + orig_metrics)\n",
        "    ext_rows.append([r[\"URL_ID\"], r[\"URL\"]] + ext_metrics)\n",
        "    time.sleep(0.2)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# Save XLSX\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "\n",
        "# Save CSV\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 9. VERIFY OUTPUT\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Extended CSV/XLSX generated successfully with at least 100 rows\")\n",
        "print(\"Sample data from extended metrics:\")\n",
        "print(df_extended.head(3))"
      ],
      "metadata": {
        "id": "X5O4jAYrptLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL (COLAB ONLY)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def resolve_url(url):\n",
        "    \"\"\"Follow redirects\"\"\"\n",
        "    try:\n",
        "        return requests.get(url, timeout=10, allow_redirects=True).url\n",
        "    except:\n",
        "        return url\n",
        "\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags\"\"\"\n",
        "    try:\n",
        "        url = resolve_url(url)\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    \"\"\"Return zeros if analysis fails\"\"\"\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FUNCTION TO GET 200 VALID ARTICLES\n",
        "# ============================================================\n",
        "def fetch_valid_articles(topic=\"artificial intelligence\", target_count=200):\n",
        "    urls, texts = [], []\n",
        "    attempt = 0\n",
        "    while len(texts) < target_count:\n",
        "        rss = f\"https://news.google.com/rss/search?q={topic.replace(' ','+')}\"\n",
        "        feed = feedparser.parse(rss)\n",
        "        candidates = [entry.link for entry in feed.entries]\n",
        "        for url in candidates:\n",
        "            if url in urls:  # skip duplicates\n",
        "                continue\n",
        "            text = extract_article_text(url)\n",
        "            if len(text.split()) > 50:  # only accept articles with >50 words\n",
        "                urls.append(url)\n",
        "                texts.append(text)\n",
        "            if len(texts) >= target_count:\n",
        "                break\n",
        "        attempt += 1\n",
        "        if len(texts) < target_count:\n",
        "            topic = \"technology\" if topic==\"artificial intelligence\" else \"science\"\n",
        "            print(f\"Not enough valid articles, changing topic to: {topic}\")\n",
        "        if attempt > 5:\n",
        "            break\n",
        "    return urls[:target_count], texts[:target_count]\n",
        "\n",
        "# ============================================================\n",
        "# 2. FETCH ARTICLES\n",
        "# ============================================================\n",
        "urls, texts = fetch_valid_articles(topic=\"artificial intelligence\", target_count=200)\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url] + ext_metrics)\n",
        "    time.sleep(0.1)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SAVE XLSX + CSV\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5. VERIFY\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Sample of extended metrics:\")\n",
        "print(df_extended.head(3))"
      ],
      "metadata": {
        "id": "FC0mfabIp4CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES (Colab only)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FETCH 200 REAL ARTICLES\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url] + ext_metrics)\n",
        "    time.sleep(0.1)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SAVE XLSX + CSV\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5. VERIFY\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Sample of extended metrics:\")\n",
        "print(df_extended.head(60))"
      ],
      "metadata": {
        "id": "GNMp8physA_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check total rows\n",
        "print(\"Total articles processed:\", len(df_extended))\n",
        "\n",
        "# Show last 5 rows\n",
        "print(df_extended.tail())\n",
        "\n",
        "# Show a summary of extended metrics\n",
        "print(df_extended.describe())"
      ],
      "metadata": {
        "id": "tbdTKCLlsnfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubatDqYJstqE",
        "outputId": "8e8c9e25-171d-43ce-e610-40a3a71ea76e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES (Colab only)\n",
        "# ============================================================\n",
        "# !pip install pandas feedparser requests beautifulsoup4 nltk openpyxl\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# ORIGINAL METRICS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# EXTENDED METRICS\n",
        "# ============================================================\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMNS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FETCH 200 REAL ARTICLES\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    # BBC Technology\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    # The Verge\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    # Reuters Technology\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\",\n",
        "    # New York Times Technology\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    # Wired\n",
        "    \"https://www.wired.com/feed/rss\",\n",
        "    # Ars Technica\n",
        "    \"http://feeds.arstechnica.com/arstechnica/index\",\n",
        "    # TechCrunch\n",
        "    \"http://feeds.feedburner.com/TechCrunch/\",\n",
        "    # CNET\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    # Engadget\n",
        "    \"https://www.engadget.com/rss.xml\",\n",
        "    # MIT Technology Review\n",
        "    \"https://www.technologyreview.com/feed/\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url] + ext_metrics)\n",
        "    time.sleep(0.1)  # polite scraping\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# 4. SAVE XLSX + CSV\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 5. VERIFY\n",
        "# ============================================================\n",
        "print(\"Original columns:\", list(df_output.columns))\n",
        "print(\"Extended columns:\", list(df_extended.columns))\n",
        "print(\"Sample of extended metrics:\")\n",
        "print(df_extended.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eLcZiuztP2Y",
        "outputId": "6f2efe59-3652-45df-d21e-c89c622e5f70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 200 valid articles with text >50 words\n",
            "Original columns: ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
            "Extended columns: ['URL_ID', 'URL', 'DOMAIN', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'NEUTRAL SCORE', 'POLARITY SCORE', 'SENTIMENT INTENSITY', 'EMOTIONALITY SCORE', 'SUBJECTIVITY SCORE', 'WORD COUNT', 'UNIQUE WORD COUNT', 'TYPE TOKEN RATIO', 'SENTENCE COUNT', 'AVG SENTENCE LENGTH', 'PARAGRAPH COUNT', 'COMPLEX WORD COUNT', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'FLESCH READING EASE', 'READING TIME (MIN)', 'PERSONAL PRONOUNS', 'QUESTION COUNT', 'EXCLAMATION COUNT', 'AVG WORD LENGTH', 'ARTICLE LENGTH CATEGORY']\n",
            "Sample of extended metrics:\n",
            "     URL_ID                                                URL  DOMAIN  \\\n",
            "0  AUTO_001  https://www.bbc.com/news/articles/crrn054nxe7o...       0   \n",
            "1  AUTO_002  https://www.bbc.com/news/articles/crmlnmnwzk2o...       0   \n",
            "2  AUTO_003  https://www.bbc.com/news/articles/c0jv1vd571wo...       0   \n",
            "\n",
            "   POSITIVE SCORE  NEGATIVE SCORE  NEUTRAL SCORE  POLARITY SCORE  \\\n",
            "0               0               0              0               0   \n",
            "1               0               0              0               0   \n",
            "2               0               0              0               0   \n",
            "\n",
            "   SENTIMENT INTENSITY  EMOTIONALITY SCORE  SUBJECTIVITY SCORE  ...  \\\n",
            "0                    0                   0                   0  ...   \n",
            "1                    0                   0                   0  ...   \n",
            "2                    0                   0                   0  ...   \n",
            "\n",
            "   COMPLEX WORD COUNT  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
            "0                   0                            0          0   \n",
            "1                   0                            0          0   \n",
            "2                   0                            0          0   \n",
            "\n",
            "   FLESCH READING EASE  READING TIME (MIN)  PERSONAL PRONOUNS  QUESTION COUNT  \\\n",
            "0                    0                   0                  0               0   \n",
            "1                    0                   0                  0               0   \n",
            "2                    0                   0                  0               0   \n",
            "\n",
            "   EXCLAMATION COUNT  AVG WORD LENGTH  ARTICLE LENGTH CATEGORY  \n",
            "0                  0                0                        0  \n",
            "1                  0                0                        0  \n",
            "2                  0                0                        0  \n",
            "\n",
            "[3 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check total rows\n",
        "print(\"Total articles processed:\", len(df_extended))\n",
        "\n",
        "# Show last 5 rows\n",
        "print(df_extended.tail(2))\n"
      ],
      "metadata": {
        "id": "frIYOkEMt4lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD OUTPUT FILES\n",
        "# ============================================================\n",
        "df_orig = pd.read_excel(\"Output.xlsx\")\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. MERGE ON URL_ID\n",
        "# ============================================================\n",
        "df_compare = pd.merge(df_orig, df_ext, on=\"URL_ID\", suffixes=(\"_orig\", \"_ext\"))\n",
        "\n",
        "# ============================================================\n",
        "# 3. COMPUTE DIFFERENCES\n",
        "# ============================================================\n",
        "metrics_to_compare = [\n",
        "    (\"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\",\n",
        "     \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\",\n",
        "     \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"WORD COUNT\")\n",
        "]\n",
        "\n",
        "# We'll store the differences as new columns\n",
        "for metric in metrics_to_compare[0]:\n",
        "    df_compare[f\"{metric}_diff\"] = df_compare[f\"{metric}_ext\"] - df_compare[f\"{metric}_orig\"]\n",
        "\n",
        "# ============================================================\n",
        "# 4. SUMMARY STATISTICS\n",
        "# ============================================================\n",
        "summary = df_compare[[f\"{m}_diff\" for m in metrics_to_compare[0]]].describe()\n",
        "print(\"Comparison Summary (Extended - Original):\")\n",
        "print(summary)\n",
        "\n",
        "# ============================================================\n",
        "# 5. PLOT COMPARISON\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_compare[0]):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    plt.scatter(df_compare[f\"{metric}_orig\"], df_compare[f\"{metric}_ext\"], alpha=0.6)\n",
        "    plt.plot([df_compare[f\"{metric}_orig\"].min(), df_compare[f\"{metric}_orig\"].max()],\n",
        "             [df_compare[f\"{metric}_orig\"].min(), df_compare[f\"{metric}_orig\"].max()],\n",
        "             color='red', linestyle='--')  # 45-degree line\n",
        "    plt.xlabel(\"Original\")\n",
        "    plt.ylabel(\"Extended\")\n",
        "    plt.title(metric)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 6. OPTIONAL: AVERAGE IMPROVEMENT\n",
        "# ============================================================\n",
        "avg_improvements = df_compare[[f\"{m}_diff\" for m in metrics_to_compare[0]]].mean()\n",
        "print(\"\\nAverage Improvement (Extended - Original):\")\n",
        "print(avg_improvements)"
      ],
      "metadata": {
        "id": "oqyrbxaNuVfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD EXTENDED OUTPUT\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SUMMARY STATISTICS OF EXTENDED-ONLY METRICS\n",
        "# ============================================================\n",
        "extended_only_metrics = [\n",
        "    \"UNIQUE WORD COUNT\", \"TYPE TOKEN RATIO\", \"FLESCH READING EASE\",\n",
        "    \"FOG INDEX\", \"READING TIME (MIN)\", \"QUESTION COUNT\",\n",
        "    \"EXCLAMATION COUNT\", \"COMPLEX WORD COUNT\"\n",
        "]\n",
        "\n",
        "print(\"Summary statistics for extended metrics:\")\n",
        "print(df_ext[extended_only_metrics].describe())\n",
        "\n",
        "# ============================================================\n",
        "# 3. DISTRIBUTION PLOTS\n",
        "# ============================================================\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(18,12))\n",
        "\n",
        "for i, metric in enumerate(extended_only_metrics):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    sns.histplot(df_ext[metric], kde=True, color=\"skyblue\", bins=20)\n",
        "    plt.title(metric)\n",
        "    plt.xlabel(\"\")\n",
        "    plt.ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 4. ARTICLE LENGTH CATEGORY DISTRIBUTION\n",
        "# ============================================================\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"ARTICLE LENGTH CATEGORY\", data=df_ext, palette=\"Set2\")\n",
        "plt.title(\"Distribution of Article Length Categories\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 5. SCATTER PLOTS TO SHOW RELATIONSHIPS\n",
        "# ============================================================\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# TYPE TOKEN RATIO vs FOG Index\n",
        "plt.subplot(1,2,1)\n",
        "sns.scatterplot(x=\"TYPE TOKEN RATIO\", y=\"FOG INDEX\", data=df_ext)\n",
        "plt.title(\"Type Token Ratio vs FOG Index\")\n",
        "plt.xlabel(\"Type Token Ratio\")\n",
        "plt.ylabel(\"FOG Index\")\n",
        "\n",
        "# Flesch Reading Ease vs Reading Time\n",
        "plt.subplot(1,2,2)\n",
        "sns.scatterplot(x=\"FLESCH READING EASE\", y=\"READING TIME (MIN)\", data=df_ext)\n",
        "plt.title(\"Flesch Reading Ease vs Reading Time\")\n",
        "plt.xlabel(\"Flesch Reading Ease\")\n",
        "plt.ylabel(\"Reading Time (minutes)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 6. TOP 10 ARTICLES BY COMPLEX WORD COUNT\n",
        "# ============================================================\n",
        "top_complex = df_ext.sort_values(\"COMPLEX WORD COUNT\", ascending=False).head(10)\n",
        "print(\"Top 10 articles by Complex Word Count:\")\n",
        "print(top_complex[[\"URL_ID\",\"URL\",\"COMPLEX WORD COUNT\",\"FOG INDEX\",\"TYPE TOKEN RATIO\"]])"
      ],
      "metadata": {
        "id": "_YbG2lCuuiLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL (Colab Only)\n",
        "# ============================================================\n",
        "# !pip install wordcloud matplotlib seaborn\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# LOAD EXTENDED OUTPUT\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. GENERATE WORD CLOUD\n",
        "# ============================================================\n",
        "# Combine all article text\n",
        "all_text = \" \".join(df_ext['URL'])  # If you saved the actual text in a column, replace 'URL' with 'TEXT'\n",
        "\n",
        "# If text column is not saved, we need to loop over URLs to extract text\n",
        "# For demonstration, assuming text is in df_ext['ARTICLE_TEXT']\n",
        "try:\n",
        "    all_text = \" \".join(df_ext['ARTICLE_TEXT'])\n",
        "except:\n",
        "    print(\"No text column found; word cloud will use URLs as placeholder\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud = WordCloud(width=1200, height=600, background_color='white', collocations=False).generate(all_text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of All Articles\", fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 2. FIND MOST COMPLEX ARTICLES\n",
        "# ============================================================\n",
        "# Use COMPLEX WORD COUNT or FOG INDEX as complexity metric\n",
        "df_ext['COMPLEXITY_SCORE'] = df_ext['COMPLEX WORD COUNT'] * df_ext['FOG INDEX']\n",
        "\n",
        "# Top 10 most complex articles\n",
        "top_complex = df_ext.sort_values('COMPLEXITY_SCORE', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 Most Complex Articles:\")\n",
        "print(top_complex[['URL_ID','URL','COMPLEX WORD COUNT','FOG INDEX','COMPLEXITY_SCORE']])\n"
      ],
      "metadata": {
        "id": "9LhdOQcJvD7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas feedparser requests beautifulsoup4 nltk openpyxl wordcloud seaborn matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlL1PYSptqgr",
        "outputId": "c0251c7e-9c1c-4d73-d4f4-3a0e92ab22d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.12/dist-packages (6.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "WrqG84uPuofN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "# ============================================================\n",
        "# NLTK SETUP\n",
        "# ============================================================\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "def extract_article_text(url):\n",
        "    \"\"\"Extract text from <p> tags of a real article page\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        html = requests.get(url, headers=headers, timeout=10).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_tokens(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1,len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith((\"es\",\"ed\")):\n",
        "        count -= 1\n",
        "    return max(count,1)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    return len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "def safe_analyze(func, *args):\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        assert result is not None\n",
        "        return result\n",
        "    except:\n",
        "        if func.__name__ == \"analyze_original\":\n",
        "            return [0]*13\n",
        "        else:\n",
        "            return [0]*24\n",
        "\n",
        "# ============================================================\n",
        "# METRIC FUNCTIONS\n",
        "# ============================================================\n",
        "def analyze_original(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    s = sia.polarity_scores(text)\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "\n",
        "    return [\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['compound'],\n",
        "        s['pos'] + s['neg'],\n",
        "        avg_sentence_len,\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        avg_sentence_len,\n",
        "        len(complex_words),\n",
        "        wc,\n",
        "        sum(count_syllables(w) for w in tokens)/wc if wc else 0,\n",
        "        count_pronouns(text),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0\n",
        "    ]\n",
        "\n",
        "def analyze_extended(text, url):\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
        "    tokens = clean_tokens(text)\n",
        "\n",
        "    wc = len(tokens)\n",
        "    sc = len(sentences)\n",
        "    pc = len(paragraphs)\n",
        "\n",
        "    s = sia.polarity_scores(text)\n",
        "    unique_words = len(set(tokens))\n",
        "    complex_words = [w for w in tokens if count_syllables(w) > 2]\n",
        "    syllables = sum(count_syllables(w) for w in tokens)\n",
        "\n",
        "    avg_sentence_len = wc / sc if sc else 0\n",
        "    pct_complex = len(complex_words)/wc if wc else 0\n",
        "    flesch = 206.835 - 1.015*avg_sentence_len - 84.6*(syllables/wc) if wc else 0\n",
        "\n",
        "    return [\n",
        "        urlparse(url).netloc,\n",
        "        s['pos'],\n",
        "        s['neg'],\n",
        "        s['neu'],\n",
        "        s['compound'],\n",
        "        abs(s['compound']),\n",
        "        s['pos'] + s['neg'],\n",
        "        (s['pos'] + s['neg'])/(wc+1e-6),\n",
        "        wc,\n",
        "        unique_words,\n",
        "        unique_words/wc if wc else 0,\n",
        "        sc,\n",
        "        avg_sentence_len,\n",
        "        pc,\n",
        "        len(complex_words),\n",
        "        pct_complex,\n",
        "        0.4*(avg_sentence_len + pct_complex),\n",
        "        flesch,\n",
        "        wc/225,\n",
        "        count_pronouns(text),\n",
        "        text.count(\"?\"),\n",
        "        text.count(\"!\"),\n",
        "        sum(len(w) for w in tokens)/wc if wc else 0,\n",
        "        \"Short\" if wc<500 else \"Medium\" if wc<=1200 else \"Long\"\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COLUMN DEFINITIONS\n",
        "# ============================================================\n",
        "original_cols = [\n",
        "    \"URL_ID\",\"URL\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "extended_cols = [\n",
        "    \"URL_ID\",\"URL\",\"ARTICLE_TEXT\",\"DOMAIN\",\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"NEUTRAL SCORE\",\n",
        "    \"POLARITY SCORE\",\"SENTIMENT INTENSITY\",\"EMOTIONALITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\"WORD COUNT\",\"UNIQUE WORD COUNT\",\"TYPE TOKEN RATIO\",\n",
        "    \"SENTENCE COUNT\",\"AVG SENTENCE LENGTH\",\"PARAGRAPH COUNT\",\n",
        "    \"COMPLEX WORD COUNT\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
        "    \"FLESCH READING EASE\",\"READING TIME (MIN)\",\"PERSONAL PRONOUNS\",\n",
        "    \"QUESTION COUNT\",\"EXCLAMATION COUNT\",\"AVG WORD LENGTH\",\n",
        "    \"ARTICLE LENGTH CATEGORY\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# RSS FEEDS (Expanded for 200+ articles)\n",
        "# ============================================================\n",
        "RSS_FEEDS = [\n",
        "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\",\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    \"https://www.wired.com/feed/rss\",\n",
        "    \"http://feeds.arstechnica.com/arstechnica/index\",\n",
        "    \"http://feeds.feedburner.com/TechCrunch/\",\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    \"https://www.engadget.com/rss.xml\",\n",
        "    \"https://www.technologyreview.com/feed/\",\n",
        "    \"https://www.reuters.com/rssFeed/technologyNews\"\n",
        "]\n",
        "\n",
        "MAX_ARTICLES = 200\n",
        "urls, texts = [], []\n",
        "\n",
        "for feed_url in RSS_FEEDS:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        url = entry.link\n",
        "        if url in urls:\n",
        "            continue\n",
        "        text = extract_article_text(url)\n",
        "        if len(text.split()) > 50:\n",
        "            urls.append(url)\n",
        "            texts.append(text)\n",
        "        if len(urls) >= MAX_ARTICLES:\n",
        "            break\n",
        "    if len(urls) >= MAX_ARTICLES:\n",
        "        break\n",
        "\n",
        "print(f\"Fetched {len(urls)} valid articles with text >50 words\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE INPUT.XLSX\n",
        "# ============================================================\n",
        "df_input = pd.DataFrame({\n",
        "    \"URL_ID\": [f\"AUTO_{i+1:03d}\" for i in range(len(urls))],\n",
        "    \"URL\": urls\n",
        "})\n",
        "df_input.to_excel(\"Input.xlsx\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# GENERATE METRICS\n",
        "# ============================================================\n",
        "orig_rows, ext_rows = [], []\n",
        "\n",
        "for i, (url, text) in enumerate(zip(urls, texts)):\n",
        "    orig_metrics = safe_analyze(analyze_original, text)\n",
        "    ext_metrics = safe_analyze(analyze_extended, text, url)\n",
        "    orig_rows.append([f\"AUTO_{i+1:03d}\", url] + orig_metrics)\n",
        "    ext_rows.append([f\"AUTO_{i+1:03d}\", url, text] + ext_metrics)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "df_output = pd.DataFrame(orig_rows, columns=original_cols)\n",
        "df_extended = pd.DataFrame(ext_rows, columns=extended_cols)\n",
        "\n",
        "# ============================================================\n",
        "# SAVE OUTPUT FILES\n",
        "# ============================================================\n",
        "df_output.to_excel(\"Output.xlsx\", index=False)\n",
        "df_extended.to_excel(\"Output_Extended.xlsx\", index=False)\n",
        "df_output.to_csv(\"Output.csv\", index=False)\n",
        "df_extended.to_csv(\"Output_Extended.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 1. GENERATE WORD CLOUD\n",
        "# ============================================================\n",
        "all_text = \" \".join(df_extended['ARTICLE_TEXT'])\n",
        "#wordcloud = WordCloud(width=1200, height=600, background_color='white', collocations=False).generate(all_text)\n",
        "\n",
        "#plt.figure(figsize=(15,7))\n",
        "#plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#plt.axis('off')\n",
        "#plt.title(\"Word Cloud of All Articles\", fontsize=20)\n",
        "#plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 2. MOST COMPLEX ARTICLES\n",
        "# ============================================================\n",
        "df_extended['COMPLEXITY_SCORE'] = df_extended['COMPLEX WORD COUNT'] * df_extended['FOG INDEX']\n",
        "top_complex = df_extended.sort_values('COMPLEXITY_SCORE', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 Most Complex Articles:\")\n",
        "print(top_complex[['URL_ID','URL','COMPLEX WORD COUNT','FOG INDEX','COMPLEXITY_SCORE']])"
      ],
      "metadata": {
        "id": "VITOZELhx2Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL TRANSFORMERS\n",
        "# ============================================================\n",
        "# !pip install transformers torch\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# LOAD EXTENDED DATA\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZE HUGGING FACE SENTIMENT ANALYZER\n",
        "# ============================================================\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. GET MODEL PREDICTIONS\n",
        "# ============================================================\n",
        "pred_labels = []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment with Model\"):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        pred_labels.append(\"NEUTRAL\")  # handle empty text\n",
        "    else:\n",
        "        res = sentiment_model(text[:512])[0]  # limit to first 512 tokens for speed\n",
        "        pred_labels.append(res['label'].upper())  # \"POSITIVE\" or \"NEGATIVE\"\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = pred_labels\n",
        "\n",
        "# ============================================================\n",
        "# 2. VADER SENTIMENT LABEL\n",
        "# ============================================================\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "vader_labels = []\n",
        "\n",
        "for text in df_ext['ARTICLE_TEXT']:\n",
        "    s = sia.polarity_scores(str(text))\n",
        "    vader_labels.append(\"POSITIVE\" if s['compound'] >= 0 else \"NEGATIVE\")\n",
        "\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "\n",
        "# ============================================================\n",
        "# 3. COMPUTE ACCURACY\n",
        "# ============================================================\n",
        "accuracy = (df_ext['MODEL_SENTIMENT'] == df_ext['VADER_SENTIMENT']).mean()\n",
        "print(f\"Accuracy of VADER sentiment vs pre-trained model: {accuracy*100:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. OPTIONAL: SHOW MISMATCHES\n",
        "# ============================================================\n",
        "mismatches = df_ext[df_ext['MODEL_SENTIMENT'] != df_ext['VADER_SENTIMENT']]\n",
        "print(f\"\\nNumber of mismatches: {len(mismatches)}\")\n",
        "print(mismatches[['URL_ID','URL','VADER_SENTIMENT','MODEL_SENTIMENT']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "referenced_widgets": [
            "4e85cd793fed44a5a28aa03df4df2726",
            "15cb254f81e347c19f66937b634fc8bb",
            "f180f217bc434cb8a4c2598c34face21",
            "0580e1e83109424a8baa1e54a31af2ae",
            "2080110b2705459d8f24a17b47568b70",
            "1d9f0a7c0bbf4ef0955913b08c2095ad",
            "03295715d9204a0482ae003ae618a520",
            "4e91c9b4b1634f1583c2d207ce0a4fd9",
            "9764364228b0432aa9f2997bb4cf782a",
            "fadb21e7053e4d008a14b70a9ef7001f",
            "e2c746b326254018bceaebe2ec4dab42",
            "d3ec46ed81dd496794e5d68b4dc07fd3",
            "18b42ef4707b463e8037c606c7931305",
            "a8f1e3f189174262ac1e2a848375ab96",
            "9a9d0b6dd0c44225a3661029104a6868",
            "f6b73e7ce62a4621a3b1ee9e45bda8a1",
            "579b7dea4fa442cd9a5208e2e5e30275",
            "5f5e546a7c63473a94c73c9bd5380fcd",
            "c735ae7d49fa4c40add412f72ae7ade6",
            "21a8440e7e564bf5ad5b3ac31c42cb48",
            "29060de86f204bd2967fe6b4b642368a",
            "b37cc9aec94a40ce94f257708d747b85",
            "2cb5ff851d664faaa3dc71216c3e9f66",
            "b80dd210d42b4034a63c8491b5f4a4b2",
            "7ef0c797894440b2a5ef5e9c82a743e3",
            "efe079ccf3c9437fbc9692b47de9b47e",
            "31693b7478f14e9b996657fa35459390",
            "0a71bb62a2ed4810babe3a9e64c40b5f",
            "0877926f33a4427186deb10274a516f0",
            "5c7f59cbde7a41ca88a54d21070d0017",
            "40a7837c1eb949d6a1417117f013a4d4",
            "3bdf56a5e3d54b6e9719660206760edc",
            "5267b92b976a4ddf8de8a8d72fe37eda",
            "e3f51dbbfb304a93ae60ae8d0bb08533",
            "646b6c0050794e76be74537ef6e49596",
            "5905a36bcccf44ff931d042c43c38f50",
            "bebcfe0bdb2a43e2a2bfaefe38d85e17",
            "2422c143e823476388b4e342b63e8a04",
            "2ed7d895e9f24020a7380c6f3e1c7ff0",
            "c693c9189b174639a77d7ed8b541bca1",
            "5a8c96bf6e5c4068881c7cc8d3051b80",
            "d419aec61bb8457d990565fb91af7720",
            "99875ffeb5c840edab9d913289cf8b19",
            "3d116751bed7453ba4ae335f924d9961"
          ]
        },
        "id": "0Vpj_eOJygU0",
        "outputId": "91453065-8a0e-4382-b783-5562dca3e5c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e85cd793fed44a5a28aa03df4df2726"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3ec46ed81dd496794e5d68b4dc07fd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cb5ff851d664faaa3dc71216c3e9f66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3f51dbbfb304a93ae60ae8d0bb08533"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Analyzing Sentiment with Model: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of VADER sentiment vs pre-trained model: 53.50%\n",
            "\n",
            "Number of mismatches: 93\n",
            "       URL_ID                                                URL  \\\n",
            "4    AUTO_005  https://www.bbc.com/news/articles/cvgjm5x54ldo...   \n",
            "5    AUTO_006  https://www.bbc.com/news/videos/c98p1jg3p58o?a...   \n",
            "8    AUTO_009  https://www.bbc.com/news/articles/c1dzdndzlxqo...   \n",
            "9    AUTO_010  https://www.bbc.com/news/articles/cj9rjwpvmpzo...   \n",
            "11   AUTO_012  https://www.bbc.com/news/articles/clyd1lpp1lyo...   \n",
            "..        ...                                                ...   \n",
            "189  AUTO_190  https://www.engadget.com/gaming/corsair-made-a...   \n",
            "190  AUTO_191  https://www.engadget.com/deals/elevationlabs-t...   \n",
            "192  AUTO_193  https://www.engadget.com/computing/accessories...   \n",
            "194  AUTO_195  https://www.engadget.com/mobile/smartphones/sa...   \n",
            "197  AUTO_198  https://www.engadget.com/big-tech/ces-2026-day...   \n",
            "\n",
            "    VADER_SENTIMENT MODEL_SENTIMENT  \n",
            "4          POSITIVE        NEGATIVE  \n",
            "5          POSITIVE        NEGATIVE  \n",
            "8          POSITIVE        NEGATIVE  \n",
            "9          POSITIVE        NEGATIVE  \n",
            "11         POSITIVE        NEGATIVE  \n",
            "..              ...             ...  \n",
            "189        POSITIVE        NEGATIVE  \n",
            "190        POSITIVE        NEGATIVE  \n",
            "192        POSITIVE        NEGATIVE  \n",
            "194        POSITIVE        NEGATIVE  \n",
            "197        POSITIVE        NEGATIVE  \n",
            "\n",
            "[93 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_ext['MODEL_SENTIMENT'], df_ext['VADER_SENTIMENT']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnqpyaQczBSg",
        "outputId": "696f015e-bc9c-4b67-c50e-30a33a90f07c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    NEGATIVE       0.81      0.16      0.27       106\n",
            "    POSITIVE       0.50      0.96      0.66        94\n",
            "\n",
            "    accuracy                           0.54       200\n",
            "   macro avg       0.66      0.56      0.46       200\n",
            "weighted avg       0.67      0.54      0.45       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INSTALL DEPENDENCIES (if not already installed)\n",
        "# ============================================================\n",
        "# !pip install transformers torch pandas nltk openpyxl tqdm\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# ============================================================\n",
        "# LOAD EXTENDED DATA\n",
        "# ============================================================\n",
        "df_ext = pd.read_excel(\"Output_Extended.xlsx\")\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZE SENTIMENT MODELS\n",
        "# ============================================================\n",
        "# Hugging Face pre-trained sentiment model\n",
        "model_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ============================================================\n",
        "# 1. MODEL PREDICTIONS\n",
        "# ============================================================\n",
        "model_labels = []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment with Model\"):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        model_labels.append(\"NEUTRAL\")\n",
        "    else:\n",
        "        res = model_analyzer(text[:512])[0]  # limit for speed\n",
        "        model_labels.append(res['label'].upper())\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = model_labels\n",
        "\n",
        "# ============================================================\n",
        "# 2. VADER STRONG SENTIMENT LABELS\n",
        "# ============================================================\n",
        "vader_labels = []\n",
        "\n",
        "for text in df_ext['ARTICLE_TEXT']:\n",
        "    s = sia.polarity_scores(str(text))\n",
        "    compound = s['compound']\n",
        "    if compound > 0.05:\n",
        "        vader_labels.append(\"POSITIVE\")\n",
        "    elif compound < -0.05:\n",
        "        vader_labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "\n",
        "# ============================================================\n",
        "# 3. ACCURACY AGAINST MODEL\n",
        "# ============================================================\n",
        "accuracy = (df_ext['MODEL_SENTIMENT'] == df_ext['VADER_SENTIMENT']).mean()\n",
        "print(f\"Accuracy of VADER (strong sentiment) vs pre-trained model: {accuracy*100:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. OPTIONAL: SHOW MISMATCHES\n",
        "# ============================================================\n",
        "mismatches = df_ext[df_ext['MODEL_SENTIMENT'] != df_ext['VADER_SENTIMENT']]\n",
        "print(f\"\\nNumber of mismatches: {len(mismatches)}\")\n",
        "print(mismatches[['URL_ID','URL','VADER_SENTIMENT','MODEL_SENTIMENT']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYYh7WQqzPRQ",
        "outputId": "1a9c57b5-662b-4f17-b0f6-1ce367b8ee3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Device set to use cpu\n",
            "Analyzing Sentiment with Model: 100%|██████████| 200/200 [00:53<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of VADER (strong sentiment) vs pre-trained model: 53.00%\n",
            "\n",
            "Number of mismatches: 94\n",
            "       URL_ID                                                URL  \\\n",
            "4    AUTO_005  https://www.bbc.com/news/articles/cvgjm5x54ldo...   \n",
            "5    AUTO_006  https://www.bbc.com/news/videos/c98p1jg3p58o?a...   \n",
            "8    AUTO_009  https://www.bbc.com/news/articles/c1dzdndzlxqo...   \n",
            "9    AUTO_010  https://www.bbc.com/news/articles/cj9rjwpvmpzo...   \n",
            "11   AUTO_012  https://www.bbc.com/news/articles/clyd1lpp1lyo...   \n",
            "..        ...                                                ...   \n",
            "189  AUTO_190  https://www.engadget.com/gaming/corsair-made-a...   \n",
            "190  AUTO_191  https://www.engadget.com/deals/elevationlabs-t...   \n",
            "192  AUTO_193  https://www.engadget.com/computing/accessories...   \n",
            "194  AUTO_195  https://www.engadget.com/mobile/smartphones/sa...   \n",
            "197  AUTO_198  https://www.engadget.com/big-tech/ces-2026-day...   \n",
            "\n",
            "    VADER_SENTIMENT MODEL_SENTIMENT  \n",
            "4          POSITIVE        NEGATIVE  \n",
            "5          POSITIVE        NEGATIVE  \n",
            "8          POSITIVE        NEGATIVE  \n",
            "9          POSITIVE        NEGATIVE  \n",
            "11         POSITIVE        NEGATIVE  \n",
            "..              ...             ...  \n",
            "189        POSITIVE        NEGATIVE  \n",
            "190        POSITIVE        NEGATIVE  \n",
            "192        POSITIVE        NEGATIVE  \n",
            "194        POSITIVE        NEGATIVE  \n",
            "197        POSITIVE        NEGATIVE  \n",
            "\n",
            "[94 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_ext['MODEL_SENTIMENT'], df_ext['VADER_SENTIMENT']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nJAJ0kGziSL",
        "outputId": "16e15c60-b820-4690-9c03-5fb977e9567a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    NEGATIVE       0.80      0.15      0.25       106\n",
            "     NEUTRAL       0.00      0.00      0.00         0\n",
            "    POSITIVE       0.50      0.96      0.66        94\n",
            "\n",
            "    accuracy                           0.53       200\n",
            "   macro avg       0.43      0.37      0.30       200\n",
            "weighted avg       0.66      0.53      0.44       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. GET MODEL PREDICTIONS AND VADER SCORES\n",
        "# ============================================================\n",
        "model_probs, model_labels, vader_compound, vader_labels = [], [], [], []\n",
        "\n",
        "for text in tqdm(df_ext['ARTICLE_TEXT'], desc=\"Analyzing Sentiment\"):\n",
        "    if not isinstance(text, str) or len(text.strip())==0:\n",
        "        model_probs.append(0.5)\n",
        "        model_labels.append(\"NEUTRAL\")\n",
        "        vader_compound.append(0)\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "        continue\n",
        "\n",
        "    # Hugging Face model prediction\n",
        "    res = model_analyzer(text[:512])[0]\n",
        "    model_labels.append(res['label'].upper())\n",
        "    model_probs.append(res['score'] if res['label'].upper()=='POSITIVE' else 1-res['score'])\n",
        "\n",
        "    # VADER\n",
        "    s = sia.polarity_scores(text)\n",
        "    compound = s['compound']\n",
        "    vader_compound.append(compound)\n",
        "    if compound > 0.05:\n",
        "        vader_labels.append(\"POSITIVE\")\n",
        "    elif compound < -0.05:\n",
        "        vader_labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        vader_labels.append(\"NEUTRAL\")\n",
        "\n",
        "df_ext['MODEL_SENTIMENT'] = model_labels\n",
        "df_ext['MODEL_PROB'] = model_probs\n",
        "df_ext['VADER_SENTIMENT'] = vader_labels\n",
        "df_ext['VADER_COMPOUND'] = vader_compound\n",
        "\n",
        "# ============================================================\n",
        "# 2. SCATTER PLOT: VADER compound vs MODEL probability\n",
        "# ============================================================\n",
        "#plt.figure(figsize=(10,6))\n",
        "#sns.scatterplot(x='VADER_COMPOUND', y='MODEL_PROB', hue='MODEL_SENTIMENT', data=df_ext, alpha=0.7)\n",
        "#plt.axvline(0.05, color='red', linestyle='--', label='VADER POS threshold')\n",
        "#plt.axvline(-0.05, color='blue', linestyle='--', label='VADER NEG threshold')\n",
        "#plt.xlabel(\"VADER Compound Score\")\n",
        "#plt.ylabel(\"ML Model Positive Probability\")\n",
        "#plt.title(\"VADER Compound Score vs ML Model Sentiment Probability\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 3. MISMATCHES\n",
        "# ============================================================\n",
        "mismatches = df_ext[df_ext['VADER_SENTIMENT'] != df_ext['MODEL_SENTIMENT']]\n",
        "print(f\"Total mismatches: {len(mismatches)}\")\n",
        "display_cols = ['URL_ID','URL','VADER_SENTIMENT','VADER_COMPOUND','MODEL_SENTIMENT','MODEL_PROB']\n",
        "print(mismatches[display_cols].head(10))  # show top 10 mismatches\n",
        "\n",
        "# ============================================================\n",
        "# 4. TOP 5 MOST EXTREME MISMATCHES\n",
        "# ============================================================\n",
        "# Extreme mismatch = VADER strongly opposite to model\n",
        "mismatches['EXTREME_DIFF'] = abs(mismatches['VADER_COMPOUND'] - (mismatches['MODEL_PROB']*2-1))\n",
        "top_extreme = mismatches.sort_values('EXTREME_DIFF', ascending=False).head(5)\n",
        "print(\"\\nTop 5 Extreme Mismatches:\")\n",
        "print(top_extreme[display_cols])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnwaZiXTzyfx",
        "outputId": "4ded4ba9-3a6a-480d-fb97-87aec7f23df1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Sentiment: 100%|██████████| 200/200 [00:53<00:00,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total mismatches: 94\n",
            "      URL_ID                                                URL  \\\n",
            "4   AUTO_005  https://www.bbc.com/news/articles/cvgjm5x54ldo...   \n",
            "5   AUTO_006  https://www.bbc.com/news/videos/c98p1jg3p58o?a...   \n",
            "8   AUTO_009  https://www.bbc.com/news/articles/c1dzdndzlxqo...   \n",
            "9   AUTO_010  https://www.bbc.com/news/articles/cj9rjwpvmpzo...   \n",
            "11  AUTO_012  https://www.bbc.com/news/articles/clyd1lpp1lyo...   \n",
            "12  AUTO_013  https://www.bbc.com/news/articles/c8dydlmenvro...   \n",
            "16  AUTO_017  https://www.bbc.com/news/articles/cjrjpzdzeddo...   \n",
            "17  AUTO_018  https://www.bbc.com/news/articles/cx25rled0ylo...   \n",
            "18  AUTO_019  https://www.bbc.com/news/articles/cgexp1q8wn1o...   \n",
            "19  AUTO_020  https://www.bbc.com/news/articles/clydp2ygrveo...   \n",
            "\n",
            "   VADER_SENTIMENT  VADER_COMPOUND MODEL_SENTIMENT  MODEL_PROB  \n",
            "4         POSITIVE          0.9770        NEGATIVE    0.449607  \n",
            "5         POSITIVE          0.9917        NEGATIVE    0.001790  \n",
            "8         POSITIVE          0.9953        NEGATIVE    0.016707  \n",
            "9         POSITIVE          0.9012        NEGATIVE    0.010214  \n",
            "11        POSITIVE          0.9988        NEGATIVE    0.001221  \n",
            "12        POSITIVE          0.7608        NEGATIVE    0.022019  \n",
            "16        POSITIVE          0.9855        NEGATIVE    0.001020  \n",
            "17        POSITIVE          0.9889        NEGATIVE    0.011064  \n",
            "18        POSITIVE          0.9875        NEGATIVE    0.028265  \n",
            "19        POSITIVE          0.8673        NEGATIVE    0.004088  \n",
            "\n",
            "Top 5 Extreme Mismatches:\n",
            "       URL_ID                                                URL  \\\n",
            "102  AUTO_103  https://www.wired.com/story/oura-whoop-blood-l...   \n",
            "11   AUTO_012  https://www.bbc.com/news/articles/clyd1lpp1lyo...   \n",
            "101  AUTO_102  https://www.wired.com/story/handy-free-speech-...   \n",
            "38   AUTO_039  https://www.bbc.com/news/articles/c24l223d9n7o...   \n",
            "52   AUTO_053  https://www.theverge.com/tech/856225/power-ban...   \n",
            "\n",
            "    VADER_SENTIMENT  VADER_COMPOUND MODEL_SENTIMENT  MODEL_PROB  \n",
            "102        POSITIVE          0.9981        NEGATIVE    0.000461  \n",
            "11         POSITIVE          0.9988        NEGATIVE    0.001221  \n",
            "101        POSITIVE          0.9967        NEGATIVE    0.000275  \n",
            "38         POSITIVE          0.9979        NEGATIVE    0.002127  \n",
            "52         POSITIVE          0.9942        NEGATIVE    0.000456  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-2562179212.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mismatches['EXTREME_DIFF'] = abs(mismatches['VADER_COMPOUND'] - (mismatches['MODEL_PROB']*2-1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IoiYJaI0xrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}